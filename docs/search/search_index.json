{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation ZDZW","text":""},{"location":"WP04/AE_IA/","title":"AE_IA: Acoustic Emissions Integrity Assesment","text":""},{"location":"WP04/AE_IA/#general-description","title":"General Description","text":"<p>The app will allow end-users to monitor the internal changes of a component using acoustic emissions (AE) technology. Acoustic events are generated inside materials when damage, phase transformations or other changes occur. The analysis of the waves associated with these events can provide information about the nature and the severity of the changes. This app will allow the user to read, process and analyze AE data produced during the inspection of a component, and apply models to evaluate its integrity.</p>"},{"location":"WP04/AE_IA/#top-ten-functionalities","title":"Top Ten Functionalities","text":""},{"location":"WP04/AE_IA/#list-of-top-ten-functionalities","title":"List of Top Ten Functionalities","text":"<ol> <li> <p>Martensite estimation using AE-AI models </p> <p>The amount of martensite generated on a sample subjected to heat treatment can be estimated using models based on acoustic emissions data. </p> </li> <li> <p>Specimen quality assessment</p> <p>Trained models are leveraged to perform quality assessment of the samples and tag them as valid or not given a predefined criteria.</p> </li> <li> <p>AI model training from experimental data\u200b</p> <p>AI models based on acoustic emissions experimental data can be trained using a set of algorithms.</p> </li> <li> <p>AI model loading/saving from experimental data </p> <p>The trained models can be saved and loaded to and from storage.</p> </li> <li> <p>AE data loading\u200b</p> <p>Acoustic emissions data files can be loaded from storage.</p> </li> <li> <p>AE feature graphical visualization </p> <p>Acoustic emissions data can be displayed using different types of graphs.</p> </li> <li> <p>AE advanced transient wave spectral analysis </p> <p>Analysis of acoustic emissions data can be performed using frequency domain algorithms.</p> </li> <li> <p>AE advanced transient wave time-domain analysis </p> <p>Analysis of acoustic emissions data can be performed using time domain algorithms.</p> </li> <li> <p>Cloud based data management </p> <p>Data is stored using cloud technology.</p> </li> <li> <p>AE data filtering tools</p> <p>Multiple filter algorithms can be applied to discriminate valuable acoustic emissions data.</p> </li> </ol>"},{"location":"WP04/AE_IA/#list-of-app-functions","title":"List of App Functions:","text":"ID System Function Function description AE_F_01 AE Data Loading Loading of AE feature and transient data, and inspection files. AE_F_02 Filtering Filtering and aggregation of data according to relevant  variables of an AE feature dataset. AE_F_03 AE features: descriptive analytics Calculation of descriptive statistics and derived quantities from AE features data. AE_F_04.1 AE Transient wave analytics: frequency domain Analysis of AE transient data using frequency domain algorithms. AE_F_04.2 AE Transient wave analytics: time domain Analysis of AE transient data using time domain algorithms. AE_F_05.1 AE Model training engine Training of AI/statistical models based on AE data. AE_F_05.2 AE Model prediction Prediction of martensited content using AI/statistical models based on AE data. AE_F_05.3 AE Model: IO Saving and loading of AI/statistical models based on AE data. AE_F_06 Quality Assesment Assessment of the quality of an specimen using the predictions of an AE model and a set of target requirements. AE_F_07 AE Visualization Display AE data and results."},{"location":"WP04/AE_IA/#architecture-diagram","title":"Architecture Diagram","text":"<p>The high level Architecture diagram for the application:</p> <p>"},{"location":"WP04/AE_IA/#image-overview","title":"Image Overview","text":""},{"location":"WP04/AE_IA/#screenshots-from-the-mockups","title":"Screenshots from the mockups","text":"<ul> <li> <p>Mock-ups data load</p> <p> <li> <p>Mock-ups analytics and visualization</p> <p> <li> <p>Mock-ups model training</p> <p> <li> <p>Mock-ups model prediction</p> <p>"},{"location":"WP04/AE_IA/#screenshots-from-the-app","title":"Screenshots from the app","text":"<ul> <li> <p>Data load page</p> <p> <li> <p>Analytics and visualization page</p> <p>"},{"location":"WP04/AE_IA/#hardware-components","title":"Hardware Components","text":"<p>The recommended method to install and run the app is by building a Docker image and running the application as a Docker container. In that case, the hardware requirements are given by Docker:</p> <p>https://docs.docker.com/desktop/install/windows-install/</p> <p>https://docs.docker.com/desktop/install/mac-install/</p> <p>https://docs.docker.com/desktop/install/linux-install/</p>"},{"location":"WP04/AE_IA/#computation-requirements","title":"Computation Requirements","text":"<p>The recommended method to install and run the app is by building a Docker image and running the application as a Docker container. In that case, the computation requirements are given by Docker:</p> <p>https://docs.docker.com/desktop/install/windows-install/</p> <p>https://docs.docker.com/desktop/install/mac-install/</p> <p>https://docs.docker.com/desktop/install/linux-install/</p> <p>Additional disk space is needed to store inspection data. The amount of storage space needed will depend of the number of inspections stored. Typical maximum file size is 60 MB for AE feature data and 2 GB for AE TR data, and each inspection has a file of each type.</p>"},{"location":"WP04/AE_IA/#installation-procedure","title":"Installation Procedure","text":"<p>The preferred way to install the software is by building a Docker image. Optionally the application can be installed manually.</p>"},{"location":"WP04/AE_IA/#installation-using-docker","title":"Installation using Docker","text":"<ul> <li>Clone the repository or download the code as a zip file.</li> <li>Go to local repository folder or to the unzipped folder.</li> <li> <p>Go to the orchestration folder and build the docker image.</p> <p><code>bash cd orchestration docker-compose up --build</code></p> </li> </ul>"},{"location":"WP04/AE_IA/#manual-installation","title":"Manual installation","text":"<p>The application can also be installed by installing the backend and the frontend individually. * Backend</p> <pre><code>The backend is installed by issuing  the following commands on the app root directory:\n\n```bash\ncd subsystems/backend\npip install -r requirements.txt --no-cache-dir\n```\n</code></pre> <ul> <li> <p>Frontend</p> <p>The frontend is installed by issuing  the following commands on the app root directory:</p> <p><code>bash cd subsystems/frontend npm install --legacy-peer-deps</code></p> </li> </ul>"},{"location":"WP04/AE_IA/#installation-on-the-platform","title":"Installation on the platform","text":"<ul> <li>In the Kubernetes platform using helm charts: description of the different options</li> </ul>"},{"location":"WP04/AE_IA/#how-to-use","title":"How To Use","text":"<p>The application has to be run differently based on the installation method used.</p>"},{"location":"WP04/AE_IA/#running-the-app-using-docker","title":"Running the app using Docker","text":"<p>During the installation a docker image with the app was built. A container can be started by:</p> <pre><code>docker run ae_ia\n</code></pre> <p>On Windows, the docker desktop daemon must be running before the previous command could be issued. This command starts services for the backend and the frontend on the localhost.</p>"},{"location":"WP04/AE_IA/#running-the-app-manually","title":"Running the app manually","text":"<p>To run the app manualy the back end and the front end have to be instantiated as separate processes.</p> <ul> <li> <p>The backend is run locally by issuing the following commands. From the root directory of the app, execute:     <code>bash     cd ./subsystems/backend/     flask run</code>     The previous command runs the app on the default flask web server. For a production deployment a WSGI dedicated server should     be used instead. The application can be served on gunicorn (not compatible con windows) or waitress:</p> <ul> <li>Gunicorn <code>bash cd ./subsystems/backend/ gunicorn -b :5000 - ae_ia:app</code></li> <li>Waitress <code>bash cd ./subsystems/backend/ waitress-serve --host 127.0.0.1 --port 5000 ae_ia:app</code> This procedures deploys the backend server on the port 5000 of the local host.</li> </ul> </li> <li> <p>To start the front end on the local host, run the following commands from the root directory of the app:     <code>bash     cd ./subsystems/frontend/     npm start</code></p> </li> </ul>"},{"location":"WP04/AE_IA/#running-the-app-on-the-platform","title":"Running the app on the platform","text":""},{"location":"WP04/AE_IA/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"},{"location":"WP04/EMAT-UI/","title":"EMAT ultrasonic inspection","text":""},{"location":"WP04/EMAT-UI/#general-description","title":"General Description","text":"<p>EMAT-based solution for multi-pass weld inspection  will be integrated into two parts: * EMAT system: a group of components that integrates the inspection system. It is made from a hardware part (EMAT equipment, sensors, conditioning box, cables, etc.) and a software one (Innerspec Technologies Operative Platform that governs hardware components and the EMAT inspection). * ZDZW EMAT Weld Inspection App: application developed in ZDZW to consult and analyse EMAT inspections and monitor the welding process' quality.</p>"},{"location":"WP04/EMAT-UI/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ul> <li>In-line Multipass Weld Ultrasonic Inspection Solution: System performs non-destructive testing  to determine weld quality using Electromagnetic Acoustic Transducers (EMAT). The system detects different defects. For each weld inspected, the system  provides an immediate disposition of weld quality and archives a complete weld record for later post-analysis,  tracking, and process monitoring.</li> <li>Real-time autocalibration of the UT signal: Selfcalibrated sensor. No need for manual calibrations. The use of separate transmitters and receivers enables the self-calibration mechanism in which a controlled amount of energy is transmitted and received by the independent coils.</li> <li>Reflection and attenuation detection mode. Analysis algorithms make the most out of the captured data by providing insights based on reflected signals or the attenuations produced by anomalies in the weld.</li> <li>ASCAN, STRIP CHART, and BSCAN real-time visualization modes that enables real time visual analysis to expert operators.</li> <li>Configurable alarms for defect detection.</li> <li>Real-time data streaming.</li> <li>Integration with third-party systems.</li> <li>Digital and analog I/O.</li> <li>AI-based defects classification tool.</li> <li>Global production statistical analysis.</li> </ul>"},{"location":"WP04/EMAT-UI/#architecture-diagram","title":"Architecture Diagram","text":"<p>The proposed architectural diagram is shown:</p> <p> <p>The ZDZW EMAT weld inspection app has the following components: * Weld Inspection Solution: the user interface of the app (frontend). * API 1: module that acts as a bridge between the user interface and the backend. * ZDZW Weld Inspection: this is the core component of the app and it manages all components. * API 2: module that acts as a bridge between the core component and the EMAT Inspection system to transmit the inspection results. * EMAT Inspection system: external module that manages hardware components and stores the raw data of EMAT inspections. * API 3: module that acts as a bridge between the core component and the Analysis module to transmit the result of EMAT inspection analysis. * Analysis module: module responsible for loading and applying trained AI models to EMAT inspections. * API 4: module that acts as a bridge between the EMAT Inspection system and the Analysis module to transmit the raw data of EMAT inspections.</p>"},{"location":"WP04/EMAT-UI/#image-overview","title":"Image Overview","text":"<p>EMAT Software Images:</p> <ul> <li> <p>Main screen:  <li> <p>ASCAN visualization:</p> </li> <p> <ul> <li>ASCAN visualization:</li> </ul> <p> <p>ZDZW EMAT Welding Inspection App Mockups:</p> <ul> <li>Main screen:</li> </ul> <p> <ul> <li>Factories:</li> </ul> <p> <ul> <li>Inspections:</li> </ul> <p> <ul> <li>Analysis:</li> </ul> <p> <ul> <li>Quality:</li> </ul> <p>"},{"location":"WP04/EMAT-UI/#hardware-components","title":"Hardware Components","text":"<p>For the application to work, the following hardware components are required: a rack-mounted electronics including a data acquisition computer, a 2-channel tone amplifier and other peripheral components. Two independent channels can be used in different ways, e.g. for detection and differentiation between two classes of defects. Alternatively, two different types of EMAT wave modes can be activated simultaneously.</p> <p>Emat inspection System Equipment:</p> <ul> <li>Sensor: The Sensors provided contain interchangeable coil circuits. Different EMAT coil circuits can be used in pitch&amp;catch arrangement or pulse-echo arrangement.</li> <li>Signal Conditioning Box: Signals are passed between the connector-less EMAT coil circuit and signal conditioning box.</li> <li>Data Acquisition Cabinet: Data Acquisition Cabinet The data acquisition cabinet is an industrial enclosure that contains the data  acquisition electronics. The electronics consist of EMAT electronics, factory interface, and a computer,  monitor, and keyboard.</li> <li>EMAT Electronics: The EMAT pulsing electronics reside in the data acquisition electronics enclosure. The FPGA data  acquisition card triggers and generates the frequency of the square pulse wave that is output to electronics  that drive and amplify a signal pulse to provide a high power, high frequency toneburst that is necessary to  drive the EMAT transmit coil.</li> <li>PRIMO DACH: The PRIMO Data Acquisition and Communication Hub (DACH) is a computer that is used to control the  inspection process through a software interface that will control system timing functions, collect and analyze  data, display, and store inspection results.</li> </ul>"},{"location":"WP04/EMAT-UI/#computation-requirements","title":"Computation Requirements","text":"<p>Self contained appliance with FPGA and RAID storage provided by Innerspec. No option to load the system in external PCs.</p> <p>Web-based monitoring interface works in all connected devices with a browser. </p>"},{"location":"WP04/EMAT-UI/#installation-procedure","title":"Installation Procedure","text":"<p>Step by step on how to install the application: * In the Kubernetes platformm using helm charts: description of the different options will be provided when they become available.</p>"},{"location":"WP04/EMAT-UI/#how-to-use","title":"How To Use","text":"<p>Step by step on how to use the application</p> <p>Application under development. This section will be updated at a later stage.</p>"},{"location":"WP04/EMAT-UI/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"},{"location":"WP04/MIR-OCT_2D/","title":"MIR-OCT-2D","text":""},{"location":"WP04/MIR-OCT_2D/#general-description","title":"General Description","text":"<p>This app is based on MIR-OCT inspection. The app will allow end-users to monitor a material or a component condition. This monitoring is based on the following:  i. Surface and sub-surface visualization  ii. Automatic defect detection based on AI.</p>"},{"location":"WP04/MIR-OCT_2D/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>*OCT Visualization*</li> <li>*CNN defect localization*</li> <li>*Contrast-Brightness*</li> <li>*Speckle Reduction/Smoothing*</li> <li>*Transparency*</li> <li>*XY Slice Inspection*</li> <li>*YZ Slice Inspection*</li> <li>*XZ Slice Inspection*</li> <li>*Scale bars*</li> <li>Volume Scrolling</li> </ol>"},{"location":"WP04/MIR-OCT_2D/#architecture-diagram","title":"Architecture Diagram","text":"<p>The architecture diagram is shown in the figure. Through the GUI and its various screens, the actor communicates with the 'Defect Inspector' module, uploading the images in PNG format obtained by the MIR-OCT system. This module communicates with the preprocessing module to adjust the parameters of the images, which are sent to the AI module for defect detection and classification. The results are produced and analyzed in the Defect Inspector module, and the AI model for defect detection and classification and the results are kept in the storage. </p> <p> </p> <p> </p>"},{"location":"WP04/MIR-OCT_2D/#image-overview","title":"Image Overview","text":"<p>The App comprises a graphical user interface (GUI) composed of five windows or panels that allow interaction with the files and functionalities of the application. These windows can be selected in the left banner marked in the figure bellow and are divided into \u2018Menu\u2019, \u2018Preprocessing and visualization\u2019, \u2018Analysis\u2019, \u2018New User\u2019 and \u2018Settings\u2019. The \u2018Menu\u2019 window will allow the user to select the different sections in the app. </p> <p> </p> <p> </p> <p>The \u2018Preprocessing and visualization\u2019 window of the app at its current stage is shown in the figure. At the top of the panel is a \u2018Load OCT\u2019 button to upload the selected volume the user wants to show.  This window consist of visualization tools that comprises:  - XY, YZ and XZ slices --to search specific dimensions for interfaces and objects in the volume.  - Slider bars -- To scroll the slices and visualize them along the volume. - Brightness, Contrast, Speckle reduction and Transparency \u2013 to manipulate voxels values and suppress speckle noise for increasing local structure contrast in volume.  </p> <p>The \u2018Analysis\u2019 panel will show the tools to assess the defect detection and classification of the OCT volumes. In addition to visualizing the OCT volume in 3D, this screen shows the type of defects by class, their location in 2D images and a graph of the history of the last 10 OCT volumes analyzed. </p> <p>At the top of the panel is a \u2018Load file\u2019 button to upload the file the user wants to analyze.  The red rectangles in the figure mark the AI defect detection and classification displays (ii). Different categories of defects can be identified automatically in the volume. Examples of defect types are:  - Voids  - Pores  - Others (cracks and irregular fractures) </p> <p>At the bottom of the panel is the option of exporting the results by pushing the \u2018Export results\u2019 button. </p> <p>Finally, the \u2018New User\u2019 window allows adding new users to manage the application. The \u2018Settings\u2019 page includes the main configuration features to work with the software, such as security, privacy and display settings and preferences. </p>"},{"location":"WP04/MIR-OCT_2D/#hardware-components","title":"Hardware Components","text":"<p>Edge processing unit based on GPUs: AI inference optimized on available GPU hardware with 100% high-rate inspection. </p>"},{"location":"WP04/MIR-OCT_2D/#computation-requirements","title":"Computation Requirements","text":"<ul> <li>Classification of images to detect defects can rely on cloud services</li> <li>Possibility to store acquired data locally or in shared/cloud space</li> </ul>"},{"location":"WP04/MIR-OCT_2D/#installation-procedure","title":"Installation Procedure","text":"<p>(To be developed) Step by step on how to install the application: * Standalone * In the Kubernetes platformm using helm charts: description of the different options</p>"},{"location":"WP04/MIR-OCT_2D/#how-to-use","title":"How To Use","text":"<p>The instructions to follow for the use of the app will be defined once the application has been developed. The following can be established as a provisional guideline: 1. Installation of the app (to be developed). 2. Select the 'New user' section in the 'Menu' window of the app and register. 3. Select the 'preprocessing and visualization' section in the 'Menu' window of the app. 4. Push the 'Load OCT' button to select the volume to be analyzed. 5. In the 'preprocessing and visualization' window, scroll through the slices and edit the settings (brightness, contrast, transparency, speckle reduction) of the volume/slices. 6. Use the 'Anaylsis' window to assess the defect detection and classification of the OCT. Optionally, export the results of the analysys via the button 'Export results'.</p>"},{"location":"WP04/MIR-OCT_2D/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"},{"location":"WP04/t4-1-mhi/","title":"MHI","text":""},{"location":"WP04/t4-1-mhi/#general-description","title":"General Description","text":"<p>The application will allow end-users to monitor in-line a hardening and/or tempering process of the workpieces. Based on machine sensors measurements, a Digital twin will estimate the case depth of the workpiece. Moreover, this application will enable to monitor the sensors signals, status information, and the estimated case depth during the process time. Finally, the application will allow the users to detect the potential anomalies, such as not enough hardened depth, during the inspection of workpieces in manufacturing line.</p>"},{"location":"WP04/t4-1-mhi/#top-ten-functionalities","title":"Top Ten Functionalities","text":""},{"location":"WP04/t4-1-mhi/#list-of-top-ten-functionalities","title":"List of Top Ten Functionalities","text":"<ol> <li>In-line monitoring of several process parameters (T\u00aa, Frequency, Current\u2026)</li> <li>Data uploading to cloud</li> <li>Cloud based data management</li> <li>In-line mechanical hardness estimation in Induction hardening</li> <li>In-line mechanical hardness estimation in Induction tempering</li> <li>Set an alarm to stop the process out of quality limits</li> <li>Show the summary of the process results on interface</li> <li>Unitary reporting of process information and hardness results</li> <li>Export registered datasets for further studies</li> <li>Data acquisition for off-line retraining of the DT</li> </ol>"},{"location":"WP04/t4-1-mhi/#list-of-app-functions","title":"List of App Functions:","text":"ID System Function Function description MHI_F_01 Login User login and process data entering (ID, workpiece properties...) for traceability. Involves restricted access. MHI_F_02 Digital Twin (DT) Data Loading Loading of the library of the DTs that correspond to the process. MHI_F_03 Capture Acquisition of sensors measures. Involves that hardware connections match DT requirements. MHI_F_04 Filtering Filtering noise data and identifying the values to be used as INPUT for the DT. MHI_F_05 In-line simulation Hardness estimation in part depth based on the trained DT. MHI_F_06 MHI Quality Assessment Unitary quality assessment of workpieces using the predictions of the DT and a set of target requirements. MHI_F_07 MHI Visualisation Displaying DT characteristics, in-line sensors values and unitary reporting of hardness estimation and target requirements. MHI_F_08 Cloud based data management Real time and post-process data loading / exchanging to the cloud. MHI_F_09 MHI Results export Generating datasets, downloading files/reports and loading them to retrain DTs and other further studies."},{"location":"WP04/t4-1-mhi/#architecture-diagram","title":"Architecture Diagram","text":"<p>The high level Architecture diagram for the application:</p> <p>"},{"location":"WP04/t4-1-mhi/#image-overview","title":"Image Overview","text":""},{"location":"WP04/t4-1-mhi/#screenshots-from-the-mockups","title":"Screenshots from the mockups","text":"<ul> <li>Mock-ups configuration 1</li> </ul> <p> <ul> <li>Mock-ups configuration 2</li> </ul> <p> <ul> <li>Mock-ups monitoring</li> </ul> <p> <ul> <li>Mock-ups summary</li> </ul> <p>"},{"location":"WP04/t4-1-mhi/#screenshots-from-the-app","title":"Screenshots from the app","text":""},{"location":"WP04/t4-1-mhi/#hardware-components","title":"Hardware Components","text":"<p>The recommended method to install and run the app is by building a Docker image and running the application as a Docker container. In that case, the hardware requirements are given by Docker:</p> <p>https://docs.docker.com/desktop/install/windows-install/</p> <p>https://docs.docker.com/desktop/install/mac-install/</p> <p>https://docs.docker.com/desktop/install/linux-install/</p>"},{"location":"WP04/t4-1-mhi/#computation-requirements","title":"Computation Requirements","text":"<p>The recommended method to install and run the app is by building a Docker image and running the application as a Docker container. In that case, the computation requirements are given by Docker:</p> <p>https://docs.docker.com/desktop/install/windows-install/</p> <p>https://docs.docker.com/desktop/install/mac-install/</p> <p>https://docs.docker.com/desktop/install/linux-install/</p>"},{"location":"WP04/t4-1-mhi/#installation-procedure","title":"Installation Procedure","text":"<p>The preferred way to install the software is by building a Docker image. Optionally the application can be installed manually.</p>"},{"location":"WP04/t4-1-mhi/#installation-using-docker","title":"Installation using Docker","text":"<ul> <li>Clone the repository or download the code as a zip file.</li> <li>Go to local repository folder or to the unzipped folder.</li> <li> <p>Go to the orchestration folder and build the docker image.</p> <p><code>bash cd orchestration docker-compose up --build</code></p> </li> </ul>"},{"location":"WP04/t4-1-mhi/#manual-installation","title":"Manual installation","text":"<p>The application can also be installed by installing the backend and the frontend individually. * Backend</p> <pre><code>The backend is installed by issuing  the following commands on the app root directory:\n\n```bash\ncd subsystems/backend\npip install -r requirements.txt --no-cache-dir\n```\n</code></pre> <ul> <li> <p>Frontend</p> <p>The frontend is installed by issuing  the following commands on the app root directory:</p> <p><code>bash cd subsystems/frontend npm install --legacy-peer-deps</code></p> </li> </ul>"},{"location":"WP04/t4-1-mhi/#installation-on-the-platform","title":"Installation on the platform","text":"<ul> <li>In the Kubernetes platform using helm charts: description of the different options</li> </ul>"},{"location":"WP04/t4-1-mhi/#how-to-use","title":"How To Use","text":"<p>The application has to be run differently based on the installation method used.</p>"},{"location":"WP04/t4-1-mhi/#running-the-app-using-docker","title":"Running the app using Docker","text":"<p>During the installation a docker image with the app was built. A container can be started by:</p> <pre><code>docker run mhi\n</code></pre> <p>On Windows, the docker desktop daemon must be running before the previous command could be issued. This command starts services for the backend and the frontend on the localhost.</p>"},{"location":"WP04/t4-1-mhi/#running-the-app-manually","title":"Running the app manually","text":"<p>To run the app manualy the back end and the front end have to be instantiated as separate processes.</p> <ul> <li> <p>The backend is run locally by issuing the following commands. From the root directory of the app, execute:     <code>bash     cd ./subsystems/backend/     flask run</code>     The previous command runs the app on the default flask web server. For a production deployment a WSGI dedicated server should     be used instead. The application can be served on gunicorn (not compatible con windows) or waitress:</p> <ul> <li>Gunicorn <code>bash cd ./subsystems/backend/ gunicorn -b :5000 - mhi:app</code></li> <li>Waitress <code>bash cd ./subsystems/backend/ waitress-serve --host 127.0.0.1 --port 5000 mhi:app</code> This procedures deploys the backend server on the port 5000 of the local host.</li> </ul> </li> <li> <p>To start the front end on the local host, run the following commands from the root directory of the app:     <code>bash     cd ./subsystems/frontend/     npm start</code></p> </li> </ul>"},{"location":"WP04/t4-1-mhi/#running-the-app-on-the-platform","title":"Running the app on the platform","text":""},{"location":"WP04/t4-1-mhi/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"},{"location":"WP05/2DSi/","title":"2D Surface Inspector","text":""},{"location":"WP05/2DSi/#general-description","title":"General Description","text":"<p>The 2D Surface Inspector (2DSI) is an app that enables acquisition and analysis of 2D camera images to assess the quality and conformity of products in high throughput industrial production lines, controlling dimensional shapes of extracted features, and/or detecting anomalies in the product surface.  </p> <p>To comply with high inspection rates, high-speed frame rate acquisition cameras are used, and the image analysis algorithms can be optimized and deployed on edge processing units based on GPUs, which allow the exploitation of Artificial Intelligence models for run-time quality inspection.  </p> <p>Linked to this, a suite of web-based services, eventually offered with in-cloud flavour (SaaS), covers from the design and test of use case-based processing flows down to the (re-)training of AI neural networks for classification, defect detection, image generation, based on the acquired and annotated datasets, eventually extended with artificially generated ones.  </p> <p>Video Systems (VSYS) is the only partner responsible of the app.</p>"},{"location":"WP05/2DSi/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>High speed camera acquisition: Possibility to acquire images of more than 1000 parts/minute, to cope with high-throuput production lines. This is a crucial requirement in many manufacturing scenarios, to be able to inspect in-line 100% parts for ZD production.</li> <li>Edge processing unit based on GPUs, and AI optimization (inference, generation) based on it: Development of new (wrt to Video Systems' background) edge processing units integrating with nVidia Jetson platform GPUs: the goal is to host into a compact embedded system the possibility to run AI trained models in an optimized acceleration framework, so to be able to inspect with AI-based technology (object detection, classification, ...) 100% parts of high-throughput production lines.</li> <li>Edge devices industrial protocols capabilities (OPCUA, MODBUS IP) and automation signals: To facilitatate in-plant integration, the vision inspection system needs to support not only I/O signals but also most common industrial communication standards and protocols, like MODBSU TCP/IP and OPC UA.</li> <li>IIoT edge devices communication (MQTT, AMQP) and acquired image streaming: To extend industrial communication capabilities in the frame of IIoT, messaging protocols like MQTT and AMQP are mandatory to facilitate integration betwwen devices and cloud services. High-rate acquired images are also available as a network stream service.</li> <li>Contours/features extraction and visualization for dimensional conformity and shape controls: Libraries and functionalities to extract contours or specific features from the image of the product are the core of the application as they allow dimensional analysis and conformity and/or detection of anomalies.</li> <li>Cloud based annotating and AI training tools (classification, annotation, generation): A suite of AI tools for classification, annotation and generation will be built based on separation of frontend (React based web interfaces) and backend (Restful http based open API). This will facilitate the deployment and integration of services into ZDZW platform, in particular once the transfer from docker technology to Kubernets will take place. The whole workflow will be covered for each type of AI service, starting from classification/annotation, managing the training phase, ending with inference/generation of batched data to test the trained networks.  </li> <li>GAN based anomaly detection and data augmentation: GAN (Generative Adversarial Networks) will also be part of AI services ecosystem, as they can be naturally expolited for anomaly detection but also for data augmentation when the number of defected parts is low and augmentation is welcome to improve the training phase.</li> <li>Supervised and semi-supervised anomaly detection AI techniques: Supervised AI techniques deal with classification, object detection and segmentation to extract features and/or defects: they request an annotation/classification phase, where defects/classes should be confirmed/annotated by inspection experts using a web-based service. Semi-supervised techniques for anomaly detection involve the use of networks trained only on non-defective parts (representing normal operating conditions), so that anomalous conditions can also be identified in newly inspected images.</li> <li>Ease to deploy AI trained models in a SaaS scheme: Once AI models have been properly trained and tested, they can be saved/exported and furtherly deployed for final inspection. The deployment of models can be optimized to be on an edge unit (see functionality 2) for in-line and real time high speed inspection, but can aslo be exploited as a cloud service in an SaaS (Software-as-a-Service) scheme: in the latter case, the AI web-based inference service can be used or its backend can be interrogated through open API end points, or as an alternative, the model can be included in a more complex algortihm flow design through the utilization of the so-called inference blocks in the AV Designer service (functionality 10)</li> <li>Ease to design and deploy Artificial Vision algorithms in a SaaS scheme: Last functionality deals with the possibility to design and test AV algorithm using a web interface, by connecting inputs and outputs of processing functions represented as blocks in a diagram canvas, in order to graphically prepare a complete processing flow: once saved as an algorithm, it can be re-used to run inspections through RESTful calls to the runtime backend of this service, which acts as a cloud processing engine. Also AI models once trained can be exported and used in this schema through so-called inference blocks (see functionality 9).</li> </ol>"},{"location":"WP05/2DSi/#architecture-diagram","title":"Architecture Diagram","text":"<p>The Figure represents the high level architecture diagram of the 2D Surface Inspector application. </p> <p>The solution has been divided in two macro-areas:  </p> <ul> <li>2DSI Vision SaaS suite: it covers all the software design services, which can run on-cloud and are handled by web-based frontend interfaces, including AI services of classification, annotation, generation of images and training of related models</li> <li>z2DSI App: it is the in-line surface inspecting system, an edge acquisition and processing unit, where optimized inspecting algorithms and AI models are finally deployed  </li> </ul> <p>2DSI Vision SaaS suite is composed of the following modules:  </p> <ul> <li> <p>Runtime Designer: The Designer is a user interface used by users to develop and debug Computer Vision algorithms. The Designer displays the list of Runtime Modules\u2019 functions as building blocks, and a flow can be created from these blocks; this flow can be tested with offline images/data through the Runtime Backend and saved as a recipe in the Internal Filesystem Storage.</p> </li> <li> <p>Runtime Backend: this component receives the processing requests from the Runtime Designer and/or the Non-Destructive Inspection APIs and orchestrates the flow to sequentially call the Runtime Modules\u2019 functions. It also accesses the Internal Filesystem Storage to access recipes, trained models, and images, and it forwards them to the correct functions.</p> </li> <li> <p>Internal Filesystem Storage: private high-speed space to store images, trained models, and recipes; it is accessible from user interface. It builds upon the system level Storage based on persistent volumes data.</p> </li> <li> <p>persistent volumes Storage: low level storage area reserved at deployment.</p> </li> <li> <p>Runtime Modules: this box in the diagram encapsulates a set of Computer Vision standalone modules; a module is a package that contains a list of processing functions in a common subdomain, like Artificial Intelligence, Silhouette analysis, and Traditional Machine Vision. These modules are managed by the Runtime Backend. New modules can be developed with the use of the Module Creator. Follows a list of some Runtime Modules (divided in AI modules and \u201cClassic\u201d modules) that are used to build quality inspection applications:  </p> </li> <li> <p>Silhouette Extractor / Comparator: to extract significant product contours from digital images as a function of search processing input; to make a parametrised comparison of two contours, one usually extracted from in-field and another typically coming from a reference image or an ideal shape (circle, rectangle, \u2026).</p> </li> <li> <p>Traditional Machine Vision: this module includes classical machine vision techniques for image pre-processing and features extraction, like contour analysis, blob analysis, histogram analysis, morphological and geometric transformations, thresholding, etc.</p> </li> <li> <p>AI Image Classifier Inference: this module runs the inference using classification models trained through the AI Image Classifier service.</p> </li> <li> <p>AI Image Labeller Inference: this module runs the inference using annotation models trained through the AI Image Labeller service.</p> </li> <li> <p>AI Image GAN generation: this module runs the image generation using GAN models trained through the AI Image GAN Generator service.</p> </li> <li> <p>Module Creator: this user interface is used to develop new modules to be added to the Runtime Modules. New modules can be developed when new Computer Vision techniques are published by the community.</p> </li> <li> <p>Builder: this user interface is used to deploy an instance of the Runtime with the selected modules. It communicates with the Application Runtime APIs to deploy the new Runtime.</p> </li> <li> <p>Non-Destructive 2DSI Inspection APIs: this interface is used by zApps or other applications/solutions to run Computer Vision algorithms with runtime or historic data.</p> </li> <li> <p>Application Runtime APIs: an integration with the Application Runtime (Kubernetes) of the ZDZW platform is implemented to allow the dynamic provisioning of a selection of available modules or newly developed (through the Module Creator) ones resulting into a dynamic (size-controlled) installation of a Runtime instance (via the Builder).  </p> </li> </ul> <p>z2DSI App is more focused on the functionalities of the edge processing unit; it is composed of the following modules:  </p> <ul> <li> <p>z2DSI UI: this web-based interface allows to configure image acquisition parameters (acquisition trigger mode, capture exposure time, programmable illuminator, \u2026) as well as processing parameters and communication settings; it allows to control basic operation of the system, also visualizing raw images and results.</p> </li> <li> <p>z2DSI Processing Manager: it is the core of the embedded system, orchestrating the main data flow, firstly the image processing (two processing schemas: through GPUs-optimized AI model deployment or exploiting the SaaS interface of the 2DSI), secondly the external communication of results/alerts, through industrial standard protocols, automation signals and IIoT message publishing and listening.</p> </li> <li> <p>Edge-optimized Processing Engine: it processes captured images, exploiting GPUs acceleration for inference/generation of previously trained AI models.</p> </li> <li> <p>Inspection Hardware: this block comprises the in-field hardware, mainly the image sensor head, and the illumination device; captured images can be configured to be saved externally (shared space) or internally.</p> </li> <li> <p>Internal Image Storage: an internal area for image storage, accessible from outside.</p> </li> <li> <p>Industrial Protocols / Automation / IIoT: this deals with possible data exchange with external systems, mainly communication with PLCs, PCs, and IIoT messages.  </p> </li> </ul>"},{"location":"WP05/2DSi/#image-overview","title":"Image Overview","text":"<p>The typical flow of preparing a supervised AI model includes the phase of classifying images, or annotating and classifying features inside them using an annotation tool, the training phase, and finally the phase of using the model on a test dataset to validate its performance: the 2DSI suite of web-based tools covers all these operations. The graphical design tool to build machine vision algorithms is also shown.</p>"},{"location":"WP05/2DSi/#hardware-components","title":"Hardware Components","text":"<p>Referring to the z2DSI App block of the architectural diagram, the in-field hardware components required to realise a complete inline inspection application are mainly the Video Systems image sensor and the Video Systems edge unit: the latter is an embedded server, called Ingenium\u00ae, based on a real-time Linux OS, to manage image acquisition and processing. A new redesign of the edge unit involves the hardware integration with NVIDIA\u00ae Jetson\u2122 GPU platform, to optimize the execution of AI models, ultimately increasing the frequency of images that can be processed to enable 100% inspection on high production lines.  </p> <p>Full archiving of captured images can be supported by saving images on a shared network space of an appropriate size. </p>"},{"location":"WP05/2DSi/#computation-requirements","title":"Computation Requirements","text":"<p>Here follow details about typical recommended requirements for the standalone installation and operation of on-line services, the ones under 2DSI Vision SaaS suite in the architectural diagram, having in mind the computational needs of the trainig phase of the AI services.</p> <ul> <li>OS: XUbuntu 23.x</li> <li>CPU: 2.30GHz \u00d7 16 (i7 11G)</li> <li>RAM: 32 GB</li> <li>Storage: &gt; 512 GB</li> <li>GPU: NVIDIA RTX A2000</li> </ul>"},{"location":"WP05/2DSi/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP05/2DSi/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/2DSi/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP05/3DSi/","title":"3D Surface Inspector (T5.1 &amp; T5.2)","text":""},{"location":"WP05/3DSi/#general-description","title":"General Description","text":"<p>The 3D Surface Inspector (3DSi) is an app that enables monitoring and control of a  multicamera quality assurance device. Equipped with advanced Artificial Intelligence (AI)  algorithms and state-of-the-art technology, 3DSi perform 3D reconstruction using images  of the object from multiple angles. It empowers users to perform accurate volumetric  analysis and conduct precise surface quality checks, detecting extra or missing material  as well as scratches or any other texture defect. Ideal for industries that require  meticulous measurements and assessments, 3DSi provides comprehensive data for  informed decision-making.</p> <p>ITI is the only partner responsible of the app.</p>"},{"location":"WP05/3DSi/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li> <p>3D Alignment based on geometry and texture: Matching geometry and texture for accurate spacial correspondence with the reference model.</p> </li> <li> <p>Global and local thresholding: Define total amount and localized defects in order to reject the inspected part.</p> </li> <li> <p>Automatic measure drift detection: Store and analyze the measurements in order to detect a drift in the production quality.</p> </li> <li> <p>3D Visualization of defects: Representation of detected defects located on the 3D reconstruction of the inspected object.</p> </li> <li> <p>Global volume difference: Volumetric difference between the reconstructed object and the expected 3D shape of it, measuring extra and missing volume.</p> </li> <li> <p>Colour drift detection: Calibrated colour difference between real object and expected.</p> </li> <li> <p>Generation of 3D localized texture model: Generate a texture model related to the 3D space of the expected object.</p> </li> <li> <p>Object classification based on 3D characteristics: Reference identification using 3D attributes of the obtained reconstruction.</p> </li> <li> <p>Region of interest specification: Specify which regions of the 3D space will be analyzed with different quality thresholds.</p> </li> <li> <p>Texture difference from expected texture: Comparison of the expected texture with that obtained in the images and located in the 3D reference.</p> </li> </ol>"},{"location":"WP05/3DSi/#architecture-diagram","title":"Architecture Diagram","text":"<ul> <li>3DSi UI: This user interface facilitates interaction with the Volumetric Inspector  and the Surface Anomaly Detection solution.</li> <li>3DSi API: This module establishes connections with other components through  the web-based user interface.</li> <li>Defect detection suite: As the core component of the application, it orchestrates  the operation of all other components in response to external client requests.</li> <li>Defect detection processor: This component performs quality analysis using the  provided data.</li> <li>Model manager: Responsible for receiving models, generating necessary analysis  files, and serving them back as required.</li> <li>Storage: This component ensures persistent data storage for the solution,  including 3D models, texture models, images, and results.</li> <li>Hardware API: It serves as the interface for connecting with the physical devices  involved in the capture process.</li> <li>Inspection Hardware: This external hardware is dedicated to capturing images of  the inspected objects.</li> </ul>"},{"location":"WP05/3DSi/#image-overview","title":"Image Overview","text":""},{"location":"WP05/3DSi/#hardware-components","title":"Hardware Components","text":"<ul> <li>Calibrated multicamera device</li> <li>Lighting system</li> <li>Industrial PLC</li> </ul>"},{"location":"WP05/3DSi/#computation-requirements","title":"Computation Requirements","text":"<ul> <li>OS Required: Ubuntu 20.04</li> <li>CPU: 32 cores, 3.5GHz (intel i7 or Ryzen 9)</li> <li>RAM: 32GB</li> <li>Storage: 1TB NVME</li> </ul>"},{"location":"WP05/3DSi/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP05/3DSi/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/3DSi/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP05/AR-Enhanced-Inspection/","title":"AREI - AR Enhanced Inspection (T5.4)","text":""},{"location":"WP05/AR-Enhanced-Inspection/#general-description","title":"General Description","text":"<p>The application allows users to navigate to previously recognized defects to verify and document the automatically found issues. In order to achieve this, the application understands its own position in relation to the inspected object. This task is divided in two parts: tracking the device position in the environment and recognizing the objects pose in this environment. Defect data from the automatic defect recognition step is imported into a backend service or provided via an API. The app\u2019s spatial understanding allows the user to be guided to the defect spots. The defect information is then superimposed on the defect site, and the user can proceed with further actions, like fixing the defect or documenting it with images or textual information.</p>"},{"location":"WP05/AR-Enhanced-Inspection/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li> <p>Device tracking in the environment: In order to have a consistent environment for virtual information, the device is keeping track of its on position and orientation in the real environment. Based on this, the examined objects position can be followed as well, after aligning it to the AR environment.</p> </li> <li> <p>Geometry import to generate reference structures: In order to visualize defects on an object's surface, its geometry needs to be known. The simplest approach is to import mesh data into the application. In the pilot, this information available as a parametric modell, which needs additional work to translate it into mesh-data. This step will most likely be developed as an external (out of the application) service.</p> </li> <li> <p>Defect data import: Data is provided to the mobile application via a Restful API. This API is either provided by the data producing application, or by a data server developed for this purpose. For this, an API for defect and associated Information was created and implemented in the Client application. A data server and import tools were developed.</p> </li> <li> <p>Robot arm pose import: Further analysis of the on-site workflow revealed that the robotic arm is not available while the application is in use, requiring a different way to locate the object in the AR space.</p> </li> <li> <p>Superimposition of found defect in AR: Defect information is superimposed on to the object's surface in the AR devices 3d view. This includes markings to identify the location as well as text information to identify the defect type and ID. Color can be used to identify the defects \"State\".</p> </li> <li> <p>Navigation to defect sites: The application shall guide the user through the physical environment, in order to find the next defect location. This either is done by visually highlighting all (relevant) locations in the devices field of view, or, by actively guiding the user to a specific defect (with visual hints).</p> </li> <li> <p>User guidance: The application shall guide the user through the applications workflow - give the user hints to go through all necessary steps for usage.</p> </li> <li> <p>Documentation support: A user shall be able to leave textual information on each defect (comment). A comment can be accompanied by an image taken on the mobile device. The user can change the \"Rating\" of the Defect (eg from \"recognized\" to \"needs rework\", \"cosmetic\", \"dirt\" or similar).</p> </li> </ol>"},{"location":"WP05/AR-Enhanced-Inspection/#architecture","title":"Architecture","text":"<ul> <li>AREI Controller: This Core module orchestrates the inspection session. It loads all needed data from the corresponding services as indicated by the session service and acts as a data backend for the UI. </li> <li>AREI UI: This module combines the tracking information (spatial relation between device and tracked object) and the defect information provided via the controller to superimpose the information onto the object in the cameras field of view. It also provides the user with an interface to add additional information to recognized defects. </li> <li>Model Tracking Module: The Model Tracking Module is responsible for recognizing the inspected object in the camera images and determine an exact enough position to superimpose content on the object. For this, the user sets a coarse alignment which will be refined by the algorithm. </li> <li>Scenario Controller: Contains information about which data belongs to the (current) session. Mainly data, tracking support data and CAD Models. </li> <li>Defect Store: This module provides access to the defect data, which either can be a service holding this data, or an API to directly access it in its original location. </li> <li>Ref Object Position Information Service: This module provides access to real time information on the pose of the inspected object and support machinery. Possible data include angle of the object (roll) or pose of the support robot arm. </li> <li>Model Manager: This module provides access to the object CAD data, which either can be a service holding this data, or an API to directly access it in its original location. Depending on the Data format, an integrated conversion step might be necessary. </li> </ul>"},{"location":"WP05/AR-Enhanced-Inspection/#image-overview","title":"Image Overview","text":"<p>The Slideshow shows three scenes from the application:</p> <ul> <li>A mockup of the defect overview.</li> <li>The app showing a virtual wind-tower segment with a defect indicator in Augmented Reality.</li> <li>The app showing a virtual wind-tower segment with a recorded defect image and input for textual annotations.</li> </ul>"},{"location":"WP05/AR-Enhanced-Inspection/#hardware-components","title":"Hardware Components","text":""},{"location":"WP05/AR-Enhanced-Inspection/#mobile-device","title":"Mobile Device","text":"<p>The handhelt mobile device in this application is the Apple iPad pro, with its integrated lidar sensor. With this hardware, the necessary accuracy can be reached. After Development, lower performnace devices can be tested and might be suitable as well.</p>"},{"location":"WP05/AR-Enhanced-Inspection/#backend-service","title":"Backend Service","text":"<p>If not directly connected to an Data Source Application via the documented API, the mobile applications connects to a Backend service, which only has to handle and manage data, without huge performance requirements. The Service is delivered as a Docker Image</p>"},{"location":"WP05/AR-Enhanced-Inspection/#computation-requirements","title":"Computation Requirements","text":""},{"location":"WP05/AR-Enhanced-Inspection/#mobile-application","title":"Mobile Application","text":"<p>M1 or A12Z iPad Pro</p>"},{"location":"WP05/AR-Enhanced-Inspection/#backend-service_1","title":"Backend Service","text":"<p>Docker host with 2GB RAM + 30GB Storage. Storage has to be scaled with the data to be held in the backend.</p>"},{"location":"WP05/AR-Enhanced-Inspection/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP05/AR-Enhanced-Inspection/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/AR-Enhanced-Inspection/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP05/MIR-OCT/","title":"MIR-OCT Volumetric Inspection","text":""},{"location":"WP05/MIR-OCT/#general-description","title":"General Description","text":"<p>This app is based on MIR-OCT inspection. The app will allow end-users to monitor a material or a component condition. This monitoring is based on the following: </p> <ul> <li>Surface and sub-surface visualization </li> <li>Automatic defect detection based on AI</li> </ul>"},{"location":"WP05/MIR-OCT/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>OCT speckle noise reduction</li> <li>Volume reconstruction from OCT slices:    The pre-processing features that comprise brightness and others to manipulate voxels values and suppress speckle noise for increasing local structure contrast in volume.</li> <li>Material layer segmentation from OCT images</li> <li>Internal microstructure composition</li> <li>OCT depth estimation for characterization</li> <li>Refraction index estimation from OCT volumes</li> <li>3D defect localization</li> <li>3D defect reconstruction</li> <li>3D defect characterization</li> <li>3D visualization of defects</li> </ol>"},{"location":"WP05/MIR-OCT/#architecture-diagram","title":"Architecture Diagram","text":"<p>The architecture diagram is shown in the figure 1. Through the GUI the actor communicates with the 'Defect Inspector' module, uploading the images in PNG format obtained by the MIR-OCT system. This module communicates with the preprocessing module to adjust the parameters of the images, which are sent to the AI module for defect detection and classification. The results are produced and analyzed in the Defect Inspector module, and the AI model for defect detection and classification and the results are kept in the storage. </p> <p> </p> <p> </p> <p>The solution is composed of several blocks: </p> <ul> <li> <p>GUI: The user interface facilitates interaction with the Defect Inspector and the Volumetric Inspection solution. </p> </li> <li> <p>Preprocessing: The features that comprise are brightness, Contrast, Speckle reduction and Transparency \u2013 to manipulate voxels values and suppress speckle noise for increasing local structure contrast in volume... </p> </li> <li> <p>AI module: This module has the task to infer the image in the model according to the settings chose.  </p> </li> <li> <p>Storage: It will store the information of the defects detected. </p> </li> </ul>"},{"location":"WP05/MIR-OCT/#image-overview","title":"Image Overview","text":"<p>The App comprises a graphical user interface (GUI) composed of panels that allow interaction with the files and functionalities of the application.</p> <p> </p> <p> </p> <p>The visualization window is shown in the figure 2. The selected volume is uploaded by the user and it is shown in the visualization tool. Through four sliders, the features allow to manipulate voxels values and suppress speckle noise for increasing local structure contrast in the volume:  - Brightness - Contrast - Speckle reduction - Transparency  </p> <p>As it is shown, the visualization tool allows to easily change the brightness of the volume through the use of the slider and check how it looks after pre-processing.</p> <p>A button will be developed later to allow inference to be made on the model.</p>"},{"location":"WP05/MIR-OCT/#hardware-components","title":"Hardware Components","text":"<p>Edge processing unit based on GPUs: AI inference optimized on available GPU hardware with 100% high-rate inspection. </p>"},{"location":"WP05/MIR-OCT/#computation-requirements","title":"Computation Requirements","text":"<ul> <li>Classification of images to detect defects can rely on cloud services</li> <li>Possibility to store acquired data locally or in shared/cloud space</li> <li>Having in mind the computational needs of the trainig and inference phase of the AI services we recommended GPU with RAM  24 GB</li> </ul>"},{"location":"WP05/MIR-OCT/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/MIR-OCT/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/MIR-OCT/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP05/ai-enhanced-painting-inspector/","title":"AI-Enhanced Painting Inspector","text":""},{"location":"WP05/ai-enhanced-painting-inspector/#general-description","title":"General Description","text":"<p>Typically, computer vision systems used for production quality control are fixed in position, and products pass through the cameras\u2019 field of view on a conveyor belt. However, this approach is not feasible in cases where the objects to be inspected have very large dimensions, such as windmill towers. Moving these products through a factory presents a complex problem in itself, so the approach here relies on moving the inspection system to avoid moving the product as much as possible. </p> <p>The AI Enhanced Painting Inspector app, developed as part of the ZDZW project, is focused on this specific type of environment.</p>"},{"location":"WP05/ai-enhanced-painting-inspector/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li> <p>AI Inference on the Edge. Capability of performing AI inference in the edge device deployed at the factory.</p> </li> <li> <p>Semi-autonomous Inspection Platform. Possibility of having a platform that can be guided through the factory to inspect large objects.</p> </li> <li> <p>Compatibility with any GeniCam compliant camera.Capability of connecting with any Industrial camera which uses the GeniCam standard.</p> </li> <li> <p>Facility to deploy Artificial Vision models. Facility of improving or adapting the computer vision model to the specific problem.</p> </li> <li> <p>Screen visualization of defects. Ability to monitor the system while performing the inspection.</p> </li> <li> <p>Collect samples for fine-tuning the model. The possibility of acquiring additional images once the AI model has bee deployed. Thanks to that, novel versions of the AI model could be developing in parallel.</p> </li> <li> <p>Flexible camera-illumination setup. The ability of having adjustability of the camera-illumination setup.</p> </li> <li> <p>Provide high-speed and low latency quality feedback. Having the possibility of receive real-time feedback for the operator of the region of the object inspected.</p> </li> <li> <p>Obtain metrics to evaluate the inference accuracy of the model. Visualise and analyse statistics of the defects detected and the accuracy of the model.</p> </li> <li> <p>Industry standard communications (e.g., OPCUA).Capability to communicate with the inspection system through OPCUA.</p> </li> </ol>"},{"location":"WP05/ai-enhanced-painting-inspector/#architecture-diagram","title":"Architecture Diagram","text":"<p>The high level Architecture diagram from the Sofware Specification document</p> <p></p> <ul> <li>Painting Inspector: AI Defect detection deployment software.</li> <li>Positioning platform: The hardware that suitably positions itself relative to the surface to be inspected.</li> <li>Edge Front-end: GUI that connects through OPC-UA to the edge device, to monitor the system and visualize a live-view.</li> <li>Cameras: GenICam compliant cameras.</li> <li>Defect Database: Database that stores the defective patchs detected by the system, with their positioning and image.</li> <li>AI Training Module: The module used to train new AI models based on the images containing defects.</li> <li>Cloud Front-End: The interface that shows some statistics of the defect database.</li> </ul>"},{"location":"WP05/ai-enhanced-painting-inspector/#image-overview","title":"Image Overview","text":""},{"location":"WP05/ai-enhanced-painting-inspector/#hardware-components","title":"Hardware Components","text":"<ul> <li>AMD64 or ARM64 PC </li> <li>Cameras</li> <li>Illumination bars</li> <li>Robotic or manual ancillary</li> <li>Positioning hardware </li> </ul>"},{"location":"WP05/ai-enhanced-painting-inspector/#computation-requirements","title":"Computation Requirements","text":"<ul> <li>OS Required: Ubuntu 22.04</li> <li>CPU: Intel i5-12600K, Jetson AGX Orin</li> <li>GPU: nvidia A2000 12 GB, nvidia RTX3060, Jetson AGX Orin</li> <li>Storage: 500GB SSD</li> </ul>"},{"location":"WP05/ai-enhanced-painting-inspector/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP05/ai-enhanced-painting-inspector/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/ai-enhanced-painting-inspector/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP05/plastic-molding-inspection/","title":"Plastic Molding Inspection","text":""},{"location":"WP05/plastic-molding-inspection/#general-description","title":"General Description","text":"<p>The app will automatically monitor the quality of the production using 3D reconstruction of manufactured goods regarding geometric correctness. The object is digitally reconstructed by an array of 3D-cameras. The 3D reconstruction of the object will be aligned and compared with the object\u2019s CAD-model to determine its quality.</p> <p>Based on the automatically provided model data, the app detects the errors and can visualize the results of the comparison as a false-colour image and point to the locations of individual defects by annotating the recorded images. Detected defects are classified and stored in a history, in order to be able to monitor production quality over time.</p>"},{"location":"WP05/plastic-molding-inspection/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>Multi Camera Setup: A generic multi camera setup should be build which involves calibrating the cameras to ensure accurate and synchronized data collection. This calibration process involves determining the intrinsic and extrinsic parameters of each camera (if not provided by the camera itself). Once calibrated, the cameras should simultaneously capture data from different perspectives.</li> <li>3D-Reconstruction from multiple images: The provided depth data from different calibrated cameras should merge into a single 3D reconstruction (for example pointcloud or mesh). The aligning process can be supported by using local alignment methods like ICP to perform even better, especially to provide a more robust setup for industrial environments.</li> <li>Defect Localization: We need to localize defects in a local area and position them onto the given 3D object in a deformable object context<ul> <li>Identify the local area where the defects are located. Use a localization technique algorithms (image processing or 3D feature based like normal-maps) to precisely determine the position of the defects within the local area.</li> <li>Map the position of the defects onto the 3D object by aligning the coordinate systems of the local area and the 3D object.</li> </ul> </li> <li>Defect Characterization: Defects can be classified into various categories based on their characteristics or attributes. The most important defects must be clarified and categorized.</li> <li>Defect Visualization: This visual representation aids in quickly identifying and addressing the most critical defects. The detected defects should be visualized in a way, a human can understand. To enhance comprehension, the detected defects can be visualized using techniques such as a false color gradient or highlighting the area of the defect in the image.</li> <li>Automatic Target-/Actual-Comparison: By utilizing the provided CAD data and the scanned object data (pointcloud or mesh), an automated comparison should be performed between the target design and the actual object. This enables the identification of any deviations or variances that may exist.</li> <li>Statistical Defect Detection: Analyze the distribution of the defects in a collected dataset (per session, per month, per year...) to identify any significant differences between the detected defects and visualize them.</li> <li>Automatic Segmentation of Defect Categories: Defects can be classified into various categories based on their characteristics or attributes. The goal is to develop an automated system that can identify and segment defects, and then assign them to the appropriate category based on their characteristics. This categorization process helps in organizing and addressing defects efficiently.</li> </ol>"},{"location":"WP05/plastic-molding-inspection/#architecture-diagram","title":"Architecture Diagram","text":"<p>The system consists of six distinct components:</p> <ul> <li>Defect Detection Controller: is the central component of the application. It creates and manages instances of all other components and controls all data-flow. In particular it opens a HTTP-server to serve the Volumetric Inspection UI and a websocket-server for subsequent communication with the UI.</li> <li>Volumetric Inspection UI: is a web-GUI deployed by the Defect Detection Controller it allows a user to configure the defect detection, see current detection results or view historical detection data.</li> <li>Defect Detector: is the component responsible for defect detection and classification. It will be realized as a class in the main application.</li> <li>Model Manager: is the component responsible for loading and managing all model- (i.e. CAD-) data. It loads this data from the file-system. It will be realized as a class in the main application.</li> <li>Camera Controller: is the interface to the camera. This includes the camera driver and respective libraries.</li> <li>Inspection Hardware: comprises one or more depth-cameras. Currently supported cameras are Ensenso X36.</li> </ul>"},{"location":"WP05/plastic-molding-inspection/#image-overview","title":"Image Overview","text":"<p>The Slideshow shows three scenes from the application:</p> <ul> <li>A mockup of the target web-UI.</li> <li>The setup at the development laboratory.</li> <li>The current prototype in action.</li> </ul>"},{"location":"WP05/plastic-molding-inspection/#hardware-components","title":"Hardware Components","text":"<p>The minimum requirements are:</p> <ul> <li>An array of depth-cameras. The system has been tested with 2 Ensenso X36. The specific requirements for the cameras depend on the use-case.</li> <li>A framework for mounting the cameras.</li> <li>A PC for running the application (see Computation Requirements).</li> <li>A monitor for displaying the results.</li> </ul>"},{"location":"WP05/plastic-molding-inspection/#computation-requirements","title":"Computation Requirements","text":"<p>Plastic Molding Inspection requires at least the following hardware:</p> <ul> <li>CPU: 8 Core, 3000MHz</li> <li>RAM: 16 GB</li> <li>Space: 2 GB</li> </ul>"},{"location":"WP05/plastic-molding-inspection/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP05/plastic-molding-inspection/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP05/plastic-molding-inspection/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP06/IRTI-Component-Surface/","title":"IR Thermal Imaging Inspector","text":""},{"location":"WP06/IRTI-Component-Surface/#general-description","title":"General Description","text":"<p>IR Thermal Imaging Inspector (IRTII) is an application from which it is possible to visualize and analyze 2D thermal images obtained during industrial processes. For this purpose, both qualitative and quantitative inspection of the acquired images is offered as a web-based service (SaaS).</p> <p>Regarding qualitative inspection, a viewer is proposed that allows loading multiple images, modifying brightness and contrast and applying a noise reduction median filter.</p> <p>Regarding the quantitative analysis, it is proposed to embed a neural network capable of detecting and classifying different types of defects, if any, in the thermal images. It should be noted that the embedded deep learning based model can be retrained to be used in particular processes by means of few-shot learning or supervised learning methodologies depending on the size of the dataset and the number and type of annotations.</p>"},{"location":"WP06/IRTI-Component-Surface/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li> <p>Loading, viewing and inspection of thermal images    Multiple acquired thermal images can be uploaded for visualization and preliminary evaluation by means of a qualitative inspection of the industrial process to be evaluated.</p> </li> <li> <p>Brightness and contrast modification for qualitative inspection of thermal images    Possibility to modify the brightness and contrast of each image using a scroller with different values. This will help for a more specific qualitative evaluation, where certain areas of the thermal images can be analyzed in more depth.</p> </li> <li> <p>Thermal image preprocessing using noise reduction filters    Possibility to reduce image noise by means of different filters. Allows the user to select the most convenient filter to perform the qualitative analysis of the thermal images.</p> </li> <li> <p>Detection of defects in thermal images    Training of neural networks with thermal images with identified defects. Inference process on new images for automatic defect detection in thermal images of industrial processes.</p> </li> <li> <p>Classification of defects in thermal imaging    Training artificial intelligence models based on deep learning using images annotated to a given class. Possibility to perform inference on new images to classify them according to the types of defects on which they have been trained. </p> </li> <li> <p>Provide high-speed and low latency quality feedback    Improvement and optimization of embedded neural networks for performing inference on new thermal imaging samples. Possibility to obtain inference results in a more efficient and faster way.</p> </li> <li> <p>Analysis of the areas of interest for classification and screening    Visualization of the heat maps obtained in the inference models on the new samples. Possibility of qualitatively analyzing the areas of interest of the image for the neural network, being able to identify biases or features that help in the detection and classification of defects.</p> </li> <li> <p>Obtain metrics to evaluate the inference performance of the model    In the case of performing a neural network inference evaluation, visualization of different figures of merit at the global level and by image on the performance of the inference process.</p> </li> <li> <p>Reconsider if the model must be retrained to fine-tune    In case the inference evaluation process does not meet expectations, the model can be retrained and fine-tuned for a specific application based on the images provided.</p> </li> <li> <p>Optimise the process for on-edge retraining    Optimization of the retraining process by means of few-shot learning techniques that allow to quickly modify the neural network predictions in order to perform an adequate inference from few labeled data and short training time.</p> </li> </ol>"},{"location":"WP06/IRTI-Component-Surface/#architecture-diagram","title":"Architecture Diagram","text":"<p>The high-level diagram of the IR Thermal Imaging Inspector architecture is depicted below.</p> <p> </p> <p> </p> <p>The solution consists of the following components:</p> <ul> <li> <p>GUI: Graphical user interface where the thermal images are loaded and the defect inspector is called from which it is possible to carry out the functionalities of the application.</p> </li> <li> <p>IR Thermal Inspector: This element is the core of the app. It receives the data loaded through the GUI and is in charge of managing the user's requests in order to display the different possible outputs. </p> </li> <li> <p>Preprocessing: Module where brightness and contrast modification is applied and noise filters are applied when requested by the user. </p> </li> <li> <p>AI module: Application of the inference process by artificial intelligence models based on deep learning to thermal imaging. Depending on the requested task and the type of data, detection of defects in the images or classification of such images will be returned.</p> </li> <li> <p>Storage: Storage of the outputs obtained by the artificial intelligence models on the loaded images. Possibility of visualization of metrics to evaluate the possibility of retraining from the inferred values.</p> </li> </ul>"},{"location":"WP06/IRTI-Component-Surface/#image-overview","title":"Image Overview","text":"<p>The typical workflow is shown in the image sequence of the image overview. First the thermal images are loaded. Then one can see how several images can be loaded at the same time and how each image can be displayed. In the following sequences, you the qualitative analysis that can be performed by varying the slider values is shown. In this way, brightness, contrast and noise reduction help to sharpen the preliminary analysis. The functionalities referred to the application of embedded artificial intelligence models to obtain results and analyze the inference made will be developed during the next months.</p>"},{"location":"WP06/IRTI-Component-Surface/#hardware-components","title":"Hardware Components","text":"<p>Edge processing unit based on GPUs: AI inference optimized on available GPU hardware with 100% high-rate inspection. </p>"},{"location":"WP06/IRTI-Component-Surface/#computation-requirements","title":"Computation Requirements","text":"<ul> <li>Classification of images to detect defects can rely on cloud services</li> <li>Possibility to store acquired data locally or in shared/cloud space</li> <li>Having in mind the computational needs of the trainig and inference phase of the AI services we recommended GPU with RAM 24 GB</li> </ul>"},{"location":"WP06/IRTI-Component-Surface/#installation-procedure","title":"Installation Procedure","text":"<p>The instructions for installing the app will be defined once the application has been completed.</p>"},{"location":"WP06/IRTI-Component-Surface/#how-to-use","title":"How To Use","text":"<p>The instructions for using the app will be defined once the application has been completed.</p>"},{"location":"WP06/IRTI-Component-Surface/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials will be added once the application has been completed.</p>"},{"location":"WP06/IRTI-Thermoforming/","title":"IR Thermal Imaging Inspection for Thermoforming","text":""},{"location":"WP06/IRTI-Thermoforming/#general-description","title":"General Description","text":"<p>IR Thermal Imaging Inspection (IRTI Thermoforming) is an edge-based application to allow end-users to monitor and process the thermal images of the heated and formed plastic sheets in the thermoforming process. 2D and 3D Thermal profile information of the produced parts will be generated. By analyzing the thermal profile information of the plastic parts by using AI/ML techniques, the anomaly such as thinning, tearing on the produced parts will be detected when it occurs. </p> <p>The architecture of IRTI Thermoforming application is designed by adopting the microservice based approach which allows to create the independently deployable services working in communication with each other. Overall architecture consists of the multiple services such as 2D Processing service, 3D Processing and Reconstruction service, AI/ML service, Communication service, Management service, Calibration service and Data Handling service. This application is also an interoperable application that is capable of communicating with Digital Twin application. The thermal data of the plastic parts which is generated by 2D and 3D processing services of the application will be sent to Digital Twin application to provide the information regarding to the thermal profile distribution of the relevant plastic sheet. Moreover, this application will enable to monitor the camera parameters such as extrinsic and intrinsic parameters, status information, and images from both the stereo and thermal cameras that will contribute to the composition of the image data. Finally, this application will allow the end-users to monitor and identify 2D/3D thermal profile of the produced plastic parts and to detect the defects such as tearing, thinning during in-line inspection of the plastic sheets in thermoforming manufacturing line. </p>"},{"location":"WP06/IRTI-Thermoforming/#top-ten-functionalities","title":"Top Ten Functionalities","text":""},{"location":"WP06/IRTI-Thermoforming/#1-computer-vision-pipeline-for-thermal-image-streaming","title":"1. Computer vision pipeline for thermal image streaming","text":"<p>This part will provide to establish a pipeline for the connection with thermal cameras and thermal data flow for image processing operations.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids","title":"Associated Function IDs:","text":"<ul> <li>CONSUME_THERMAL_DATA</li> <li>CONSUME_STEREO_DATA</li> <li>START_2D_PROCESSING</li> <li>START_3D_PROCESSING</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#2-data-handling-service-for-data-analysis","title":"2. Data handling service for data analysis","text":"<p>This service will enable to process the real-time thermal images and raw data gathered from the thermal cameras.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_1","title":"Associated Function IDs:","text":"<ul> <li>INITIATE_THERMAL_CAMERA</li> <li>INITIATE_STEREO_CAMERA</li> <li>SHUTTER_CONTROLLING_THERMAL_CAMERA</li> <li>STOP_THERMAL_CAMERA</li> <li>STOP_STEREO_CAMERA</li> <li>START_IMAGE_STREAMING</li> <li>STOP_IMAGE_STREAMING</li> <li>GET_DEPTH_STEREO_CAMERA</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#3-2d-thermal-profile-identification-from-thermal-images","title":"3. 2D Thermal profile identification from thermal images","text":"<p>This service will enable to process the streaming thermal data (thermal image and raw data) to identify two-dimensional thermal profile distribution of the plastic sheets.  </p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_2","title":"Associated Function IDs:","text":"<ul> <li>START_IMAGE_STITCHING</li> <li>STOP_IMAGE_STITCHING</li> <li>EXTRACT_2D_ROI_THERMAL</li> <li>STORE_2D_OUTPUT_DATA</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#4-3d-processing-and-surface-reconstruction-from-point-cloud","title":"4. 3D Processing and Surface reconstruction from point cloud","text":"<p>3D processing and surface reconstruction service will integrate the data obtained from both the depth camera and thermal camera to construct the three-dimensional object view that incorporates the thermal information. 3D processing and surface reconstruction service will facilitate the generation of three-dimensional objects using point clouds acquired from a depth camera.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_3","title":"Associated Function IDs:","text":"<ul> <li>START_3D_RECONSTRUCTION</li> <li>FUSION_3D_AND_THERMAL_DATA</li> <li>STORE_3D_OUTPUT_DATA</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#5-ai-model-training-using-thermal-image-datasets","title":"5. AI model training using thermal image datasets","text":"<p>This part will provide ai and machine learning models training and dataset generation for thermal image processing.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_4","title":"Associated Function IDs:","text":"<ul> <li>TRAIN_2D_ANOMALY_DETECTION_MODEL</li> <li>TRAIN_3D_ANOMALY_DETECTION_MODEL</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#6-edge-based-thermal-inspection-on-the-heated-and-formed-plastic-sheets","title":"6. Edge based thermal inspection on the heated and formed plastic sheets","text":"<p>The application of IRTI Thermoforming will perform as an edge-based system to facilitate in-plant integration. </p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_5","title":"Associated Function IDs:","text":"<ul> <li>INITIATE_TRITON_INFERENCE_SERVER</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#7-aiml-service-development-for-anomaly-detection","title":"7. AI/ML service development for anomaly detection","text":"<p>AI/ML service will provide ai and machine learning operations regarding to defect detection of the heated and formed plastic parts.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_6","title":"Associated Function IDs:","text":"<ul> <li>START_2D_ANOMALY_PROCESS</li> <li>START_3D_ANOMALY_PROCESS</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#8-ui-design-for-simulation-and-monitoring","title":"8. UI design for simulation and monitoring","text":"<p>This part will handle the development of web-based UI interface for the application.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_7","title":"Associated Function IDs:","text":"<ul> <li>SHOW_THERMAL_IMAGES</li> <li>ACTIVATE_2D_PROCESSING</li> <li>ACTIVATE_3D_PROCESSING</li> <li>DEACTIVATE_2D_PROCESSING</li> <li>DEACTIVATE_3D_PROCESSING</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#9-calibration-service-for-thermal-and-stereo-cameras","title":"9. Calibration service for thermal and stereo cameras","text":"<p>This service will perform the calibration process to establish the relative position alignment between thermal and depth cameras.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_8","title":"Associated Function IDs:","text":"<ul> <li>DOWNLOAD_AUTO_CALIBRATION_FOR_THERMAL_CAMERA</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#10-development-of-closed-loop-communication-service-with-digital-twin-application","title":"10. Development of closed-loop communication service with digital twin application","text":"<p>This service will enable to establish the bi-directional communication with Digital Twin application for the notification the thermal profile distribution and detected defects on the plastic sheets to Digital Twin App.</p>"},{"location":"WP06/IRTI-Thermoforming/#associated-function-ids_9","title":"Associated Function IDs:","text":"<ul> <li>FETCHING_THERMAL_PROCESS_ID_FROM_DT</li> <li>SENDING_THERMAL_PROCESS_STATUS_TO_DT</li> <li>HEARTBEAT_SEND_TO_DT</li> <li>HEARTBEAT_RECEIVE_FROM_DT</li> <li>RAISE_ALARM</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"WP06/IRTI-Thermoforming/#the-application-consists-of-several-services-and-components","title":"The application consists of several services and components:","text":"<ul> <li>Thermal Inspection GUI: Graphical user interface for the application (front end).</li> <li>Thermal Camera Service: This service handles to receive the thermal raw data from thermal cameras and to transmit the thermal data to the corresponding services.</li> <li>Stereo Camera Service: This service is responsible for receiving data from the stereo camera and transmitting the received data between services.</li> <li>Management Service: This service aims to achieve efficient coordination of the application and management tasks within authentication, authorization, communication, and calibration to provide smooth operations of the application.</li> <li>2D Processing Service: This service enables to process the thermal images obtained from thermal cameras to generate 2D thermal profile identification. It performs advanced image processing techniques such as filtering and image fusion to gather valuable qualities and improve thermal imaging results.</li> <li>3D Processing Service: This service integrates data from both the depth camera and the thermal camera to construct a 3D object that incorporates thermal information. </li> <li>3D Reconstruction Service: This service facilitates the generation of three-dimensional objects using point clouds acquired from a depth camera.</li> <li>System Calibration Service: This service performs the calibration process to establish the relative position alignment between thermal and depth cameras.</li> <li>AI/ML Service: This service provides ai and machine learning operations.</li> <li>Communication Service: This service enables the communication with DT App.</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#image-overview","title":"Image Overview","text":"<p>Add here real screen shots from the apps or mockups from the Software Specification document</p>"},{"location":"WP06/IRTI-Thermoforming/#app-login-page","title":"App Login Page","text":""},{"location":"WP06/IRTI-Thermoforming/#configuration-page","title":"Configuration Page","text":""},{"location":"WP06/IRTI-Thermoforming/#mockup","title":"Mockup","text":""},{"location":"WP06/IRTI-Thermoforming/#implementation","title":"Implementation","text":""},{"location":"WP06/IRTI-Thermoforming/#home-page","title":"Home Page","text":""},{"location":"WP06/IRTI-Thermoforming/#mockup_1","title":"Mockup","text":""},{"location":"WP06/IRTI-Thermoforming/#thermal-profile-analysis-and-visualization-page","title":"Thermal Profile Analysis and Visualization Page","text":""},{"location":"WP06/IRTI-Thermoforming/#mockup_2","title":"Mockup","text":""},{"location":"WP06/IRTI-Thermoforming/#implementation_1","title":"Implementation","text":""},{"location":"WP06/IRTI-Thermoforming/#hardware-components","title":"Hardware Components","text":"Hardware Component Count Thermal Camera 3 Stereo Camera 1 Edge Device 2 Training/Inference Server 1 <ul> <li>Thermal Camera : Thermal cameras are used to capture thermal data from the thermoforming line during 2D/3D thermal inspection.</li> <li>Stereo Camera : Stereo cameras are used to capture point cloud data from 3D Reconstruction Phase.</li> <li>Edge Device : Edge device is used to execute specific microservices which are running custom algorithms.</li> <li>Training/Inference Server : The training/inference server is used to serve the APIs that perform the training / inference on the AI models.</li> </ul>"},{"location":"WP06/IRTI-Thermoforming/#computation-requirements","title":"Computation Requirements","text":""},{"location":"WP06/IRTI-Thermoforming/#computational-requirements-for-edge-device-edge-box","title":"Computational Requirements for Edge Device / Edge Box:","text":"Recommended OS Ubuntu Linux 22.04 LTS, x86_64 architecture CPU Intel Core i5-10th Gen or higher RAM 64 GB DDR4 3000MHz Memory or higher STORAGE 1TB PCIe M.2 NVMe SSD for OS GPU NVIDIA GeForce RTX 3090 384-bit 24GB GDDR6X (Ampere) PERIPHERALS USB 2.0 Bus x2, USB 3.2 x1"},{"location":"WP06/IRTI-Thermoforming/#computational-requirements-for-training-and-inference-server","title":"Computational Requirements for Training and Inference Server:","text":"Recommended OS Ubuntu Linux 22.04 LTS, x86_64 architecture CPU Intel Core i7-10th Gen or higher RAM 128 GB DDR4 3000MHz Memory or higher STORAGE 1TB PCIe M.2 NVMe SSD for OS GPU 4 x NVIDIA RTX A5000 24GB GDDR6 with ECC memory (Ampere) PERIPHERALS USB 2.0 Bus x2, USB 3.2 x1"},{"location":"WP06/IRTI-Thermoforming/#installation-procedure","title":"Installation Procedure","text":"<p>Step by step on how to install the application: * Standalone * In the Kubernetes platformm using helm charts: description of the different options</p>"},{"location":"WP06/IRTI-Thermoforming/#how-to-use","title":"How To Use","text":"<p>Step by step on how to use the application</p>"},{"location":"WP06/Welding-Process-Monitoring/","title":"T6.3 - Welding Process Inspector","text":""},{"location":"WP06/Welding-Process-Monitoring/#general-description","title":"General Description","text":"<p>This solution aims to predict/estimate the presence of defects during the welding process using process parameters monitored in real time. The goal is to reduce the production time and waste when a defective weld occurs.</p> <p>It is deployed in two stages. First, several sensors are selected and integrated on the welding torch to obtain a comprehensive monitoring of the welding process. The sensors will look at different process parameters like welding speed, wire feed rate, welding electrical signals, thermal info of bead and base plate\u2026 The system will synchronize the data temporally and spatially, creating valuable and complete datasets.</p> <p>On a second stage, this data is analyzed and correlated with NDT data to create data models capable to detect high probabilities of defect appearence in real time. The model is deployed following an edge computing paradigm, i.e. next to the monitoring system.</p> <p>This solution requires specific HW. The data acquisition SW and the defect detection model are embedded in the acquisition HW. The user will have two interfaces: one for the configuration and control of the HW and the other for the offline and detailed visualization of the recorded data.</p>"},{"location":"WP06/Welding-Process-Monitoring/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li> <p>Welding process monitoring system: Main feature of the Welding Process Inspector. It provides the technical means (HW &amp; SW) to monitorize all relevant parameters of a welding processassutring a good synchroniztion.</p> </li> <li> <p>Enables implementation of AI-based quality assurance algorithms: Both the HW and the SW are designed to implement quality control algorithms, feeding them with real-time processs data.</p> </li> <li> <p>Early detection of defects: Time and scrap reduction in rework tasks. The monitoring system can be used to acquire welding data. These data is susceptible to be used to elaborate a data model of the welding process that should be able to detect when the welding process is out of the optimal parameters window and, therefore, return the probability of defects appearing in time real.</p> </li> <li> <p>Flexibility: The monitoring system covers a great variety of sensors and communication protocols (e.g., TCP/IP or Profibus).</p> </li> <li> <p>Scalability: The system eases to add new sensors. The monitoring HW and SW can be updated and cofnigured to handle a great number of sensors.</p> </li> <li> <p>Adaptability: The monitoring system is adaptable to any metallic welding process. The sensors and protocols used to monitoring the welding process can be selected for each specific case (i.e. d+different welding proccesses, diferent weldign stations...)</p> </li> <li> <p>Contribution to the digitalization of the industry: Process variables are digitally acquired and stored. The monitored data can be seen as the digital twin of the welding process, enabling a digital traceability of each welded bead.</p> </li> <li> <p>Possibility to set alarms when out of quality limits: The Welding Process Inpector provides the data processing and the interface required to implement alarms when abnormal process behaviours are being produced.</p> </li> <li> <p>Enables process control: The Welding Process Inpector furnishes further data to understand reality of the welding process and provides the interfaces with the process to test/implement closed-loop control strategies.</p> </li> <li> <p>Thermal info from welding bead and base plates: The system can record images from IR cameras using GeniCam standard. Also, punctual pyrometers contribute to gather thermal information.</p> </li> </ol>"},{"location":"WP06/Welding-Process-Monitoring/#architecture-diagram","title":"Architecture Diagram","text":"<ul> <li>Internal modules:<ul> <li>Main: Synchronizes data acquisition from the different internal modules (sensors, data storage and defect detection algorithms).</li> <li>GigE IR cam: This module enables connection to cameras following GigE/genie cam protocols/standards.</li> <li>DAQ board: Data Acquisition Board. Acquire digital and analogue signals, e.g., pyrometers.</li> <li>Oscilloscope: Aaquires analogue signals at a high sampling frequency, i.e., voltage and current of each welding torches.</li> <li>Laser line: retrieves the geometrical data scanned by the laser line profilometer.</li> <li>Pose Logger: Retrieves the position of the welding torches regarding the whole part.</li> <li>Defect Detection Algorithm: Analises data stream from sensors and warns when anomalies are identified.</li> <li>Storage: Data is saved locally in a custom data format.</li> </ul> </li> <li>GUI: Graphical User Interface. Allows the user to configure the monitoring system and to check the correct operation of the system during welding.</li> <li>API: SW interface to the monitoring system</li> </ul>"},{"location":"WP06/Welding-Process-Monitoring/#image-overview","title":"Image Overview","text":"<p>The present section addresses the user interfaces envisioned to interact with the Welding Process Monitoring solution. Two GUIs are defined, one for configuring all devices involved in the data acquisition which also provides some features for online visualization and another one for offline analysis of the recorded data.</p> <p></p> <p>Above figure is a design for the GUI in charge of the HW configuration of ZDZW Welding Inspector the and a subsampled online visualization of the welding process (a.k.a. recorder app). On the left side you can see the 3 different formats for the visualization of the data that is being recorded, while the right side is reserved for the configuration of the system and to start/stop the recording. The online visualization includes an image viewer for the IR camera and graph and scalar viewers which allow the user to plot desired parameters. The \u201cconfiguring input/output interfaces\u201d contains the settings of the sensors that are being used for monitoring the welding process. The \u201cRecorded params\u201d section links the process parameters with the sensor that is being used to get their values. The \u201cmetadata\u201d section contains relevant logistic information. The GUI allows the user to enable/disable a defect detection model and to load different models. Finally, the GUI offers a couple of buttons to start and stop the recording manually.</p> <p></p> <p>Above figure shows the mockup of the GUI for the offline visualization (a.k.a. visualizer app). While the recorder app offers a restricted, but online, visualization of the monitored data from the welding process, the visualizer app allows skilled personnel to analise the recorded data aand visualize it in different formats. The recorder app allows the operator to detect disturbances on the welding process or possible mistakes in the configuration of the monitored system, while the visualizer app allows a deeper analysis of the recorded data. In addition to the image and graph viewers already available in the recorder app, the visualizer app also provides a 3D representation of the process parameters.</p>"},{"location":"WP06/Welding-Process-Monitoring/#hardware-components","title":"Hardware Components","text":"<p>The Welding Process Inspector relies on a combination of some specific HW and custom SW. The HW is divided into a CPU, some middle HW for signal processing and the sensors.</p> <p></p> <p>A small factor CPU is chosen for the sake of space economy, but guaranteeing minimum requirements to handle the data acquisition SW and capability to deploy data models based on Deep Learning techniques.</p> <p>To capture the detailed waveform of welding currents and voltages we opted for a PicoScope USB oscilloscope. When selecting this device, it is vital to ensure the proper number of channels to cover all welding current and voltage signals and a propper bandwidth to guarantee a right sample rate. These parameters may vary depending on the specific welding technique or welding procedure being monitored.</p> <p>The welding voltage must be pre-processed at the input of the oscilloscope to adapt the voltage amplitude and to isolate the signal to avoid any electromagnetic interference. To that purpose, a differential voltage probe must be selected for each welding voltage signal. The input range must cover the welding voltage range and the differential output range must fit in the oscilloscope input range.</p> <p>A DAQ (digital acquisition board) processes analog and digital signals to be read by a CPU. In our app we use a labjack T7 which handle fairly high resolution analog I/O and digital I/O. This particular device has some useful funtionalities as frequency inputs, high speed counters or support to protocols like SPI or I2C among others. The great variety of options of this DAQ contributes to several functionalities of the solution, like the flexibility, adaptability, scalability or the ability to set alarms to stop the welding process when it's out of quality limits, for example.</p> <p></p> <p>CPU and intermediate signal processing HW is enclosed in a cabinet alongside other components as power sources or wifi/4G router. The cabinet can be easily installed next to or onto the welding station. Different connectors provide HW interfaces to connect the sensors allocated in the different areas of the welding station by convenient wire hoses.</p> <p></p> <p>The sensors must be placed on the welding torch, welding power wire hose and positioning system to acquire the relevant information. Above picture show some typical sensors placed in a Sumerged Arc Welding torch: encoders for wire feed rate (green), pyrometers (red), laser-line profilometer (yellow) and GigE camera (blue).</p>"},{"location":"WP06/Welding-Process-Monitoring/#computation-requirements","title":"Computation Requirements","text":"<p>Below we detail the computation requirements for the user interfaces depicted in Image Overview section.</p> minimum recommended cpu 2.60GHz \u00d7 6 (i5 11G) 2.30GHz \u00d7 16 (i7 11G) ram 16 Gb 32 Gb storage 512 Gb &gt; 1 Tb"},{"location":"WP06/Welding-Process-Monitoring/#installation-procedure","title":"Installation Procedure","text":"<p>[ Step by step on how to install the application: * Standalone * In the Kubernetes platformm using helm charts: description of the different options ]</p> <p>TBD</p>"},{"location":"WP06/Welding-Process-Monitoring/#how-to-use","title":"How To Use","text":"<p>[Step by step on how to use the application]</p> <p>TBD</p>"},{"location":"WP06/Welding-Process-Monitoring/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>[Links to other learning materials like youtube tutorials or work from WP10]</p> <p>TBD</p>"},{"location":"WP06/htdt/","title":"# ZDZW - Heat Transfer Digital Twin (HTDT)","text":""},{"location":"WP06/htdt/#general-description","title":"General Description","text":"<p>Heat Transfer Digital Twin (HTDT) is a software package specifically designed for manufacturing operations that are based on heating and cooling processes. The system has simulation capabilities based on Finite Element Analysis on top of integrating real time Thermocouple (TC) as well as Infrared (IR) Camera readings. Specifically in ZDZW project, this package will be utilized to create a Digital Twin of Thermoforming process. </p>"},{"location":"WP06/htdt/#top-ten-functionalities","title":"Top Ten Functionalities","text":""},{"location":"WP06/htdt/#1-heat-transfer-simulation","title":"1. Heat Transfer Simulation","text":"<p>This functionallity is the related to the base module where thermal calculations are performed with given material, geometric settings and boundary conditions. This function can handle all three mode of heat transfer, namely conduction, convection (forced or free) and radiative. In thermoforming setting, mainly conduction and radiative modes are dominant. Finite Element Method is used to discretize partial differential equations.</p>"},{"location":"WP06/htdt/#associated-function-ids","title":"Associated Function IDs:","text":"<ul> <li>HTINIT, HTSOLVE, HTDT</li> </ul>"},{"location":"WP06/htdt/#2-steady-state-and-transient-formulation","title":"2. Steady State and Transient Formulation","text":"<p>This is where the temporal behaviour of the solver is set. Both steady and unsteady (i.e transient) simulations can be performed. For Steady State formulations, the material properties are not important, only the geometric setup and boundary conditions are needed. In transient formulation, time increment, material properties (density, conductivity, specific heat) and start-end time values are needed. Depending on the discretization approach, both explicit and implicit formulations can be applied.</p>"},{"location":"WP06/htdt/#associated-function-ids_1","title":"Associated Function IDs:","text":"<ul> <li>HTTIME</li> </ul>"},{"location":"WP06/htdt/#3-parameteric-geometry-definition","title":"3. Parameteric Geometry Definition","text":"<p>HTDT has the ability to create fundamental shapes that are frequently used in engineering, including boxes, parallelpiped geometris, cylinder and their variations. In thermoforming, base shape is always a rectangular prism with a thickness around 2-8 mm and width and depth ranging from 1m to 3m's. To simplfy the setup of the digital model, data can be received from factory database and reflected onto the digital model.</p>"},{"location":"WP06/htdt/#associated-function-ids_2","title":"Associated Function IDs:","text":"<ul> <li>HTINIT, HTCONNECT</li> </ul>"},{"location":"WP06/htdt/#4-parameter-tuning-for-radiative-and-material-constants","title":"4. Parameter Tuning for radiative and material constants","text":"<p>Even if raw materials are pure and delivered with pre-measured material properties, depending on the nature of the operations, changes on the equipment (aging, different season,) more realistic estimations on parametric settings are needed. This functionality is useful to approximate required parameters for HTINIT. Further, view factor estimations are also needed to perform Radiative heat transfer calculations. This function also enables the tuning for vf's.</p>"},{"location":"WP06/htdt/#associated-function-ids_3","title":"Associated Function IDs:","text":"<ul> <li>HTFIT, VFCALC</li> </ul>"},{"location":"WP06/htdt/#5-calibration-with-thermocouple-sensor-data","title":"5. Calibration with Thermocouple Sensor data","text":"<p>Numerical results achieved out of the finite model is generally for a perfect model. To capture realistic effects on the machine, a feedback system is required to collect sensor data and use it to improve the prediction. This functionality is focused in calibrating the numerical model, by measuring the temperature on a known location where the numerical value is compared to real measurement. Using this information, an information is passed to parameter settings so that it can be updated to minimize the error between the sensor data and numerical approximation.</p>"},{"location":"WP06/htdt/#associated-function-ids_4","title":"Associated Function IDs:","text":"<ul> <li>HTFIT, HTCONNECT</li> </ul>"},{"location":"WP06/htdt/#6-thermal-camera-input-for-enhanced-approximation","title":"6. Thermal Camera Input for enhanced approximation","text":"<p>Similar to TC calibration, this functionality is also focused on getting external data and using it to improve the prediction of the Digital Twin. Using IR data, instead of getting just one point data, a map of temperature readings will be received which can be used to offer a more optimized and even improvement of the thermal map.</p>"},{"location":"WP06/htdt/#associated-function-ids_5","title":"Associated Function IDs:","text":"<ul> <li>HTFIT, HTCONNECT, SIE_FUNCTIONS</li> </ul>"},{"location":"WP06/htdt/#7-optimization-based-on-temperature-target","title":"7. Optimization based on temperature target","text":"<p>This function can create a pre-defined temperature maps on the plastic sheet. Optimization is applied on the machine parameters through heater array so that the changes can be used to modify the distribution on the thermal sheet.</p>"},{"location":"WP06/htdt/#associated-function-ids_6","title":"Associated Function IDs:","text":"<ul> <li>HTOPT, HTCONNECT</li> </ul>"},{"location":"WP06/htdt/#8-optimization-based-on-cycle-time-target","title":"8. Optimization based on cycle time target","text":"<p>This function is focused to keeping the temperature map on the plastic sheet fixed but creating a setup on the machine where the cycle time can be modified depending on the target of the factory operators/engineers while changing the temporal parameters. Using this functionality, throughput on the machine can be increased.</p>"},{"location":"WP06/htdt/#associated-function-ids_7","title":"Associated Function IDs:","text":"<ul> <li>HTOPT</li> </ul>"},{"location":"WP06/htdt/#9-heater-map-tuning","title":"9. Heater Map Tuning","text":"<p>To focus on energy related KPIs, heater map array can also be tuned automatically. This is also valid for new product introduction, where a new type mold will be used on the thermoforming machine. Using this function, heat map array can be created before initiating the operation and can be updated on the fly why the system starts running.</p>"},{"location":"WP06/htdt/#associated-function-ids_8","title":"Associated Function IDs:","text":"<ul> <li>HTOPT</li> </ul>"},{"location":"WP06/htdt/#10-failed-heater-compensation","title":"10. Failed Heater Compensation","text":"<p>In cases where one or more heaters on heater-array is defective - i.e. either does not function or underperforms, then the Digital Twin stars offering an automated heater compensation where the defective heater is removed from the setup by adjusting the PLC while fixing the neighbouring heaters around the problematic element so that the targer temperature map can still be achieved while avoiding down-time for maintanance.</p>"},{"location":"WP06/htdt/#associated-function-ids_9","title":"Associated Function IDs:","text":"<ul> <li>HTOPT, HTCONNECT</li> </ul>"},{"location":"WP06/htdt/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"WP06/htdt/#image-overview","title":"Image Overview","text":""},{"location":"WP06/htdt/#initial-mockup","title":"Initial Mockup","text":""},{"location":"WP06/htdt/#real-application-in-progress","title":"Real Application (In Progress)","text":""},{"location":"WP06/htdt/#dashboard","title":"Dashboard","text":""},{"location":"WP06/htdt/#auth","title":"Auth","text":""},{"location":"WP06/htdt/#configuration","title":"Configuration","text":""},{"location":"WP06/htdt/#main-heaters","title":"Main Heaters","text":""},{"location":"WP06/htdt/#hardware-components","title":"Hardware Components","text":"<p>The Digital Twin only requires a cloud/on-prem server to run. Hardware requirement related to TC or IR data could be delivered from external libraries or services.</p>"},{"location":"WP06/htdt/#computation-requirements","title":"Computation Requirements","text":"<p>8-core Processor with 16 GB RAM and 1TB SSD with ethernet connection is required to run the system. The OS of the server should a linux, preferabble a Ubuntu Release 20+</p>"},{"location":"WP06/htdt/#installation-procedure","title":"Installation Procedure","text":"<p>Step by step on how to install the application: * Standalone dockerized system. * Manual Installation</p>"},{"location":"WP06/htdt/#manual-installation-requirements","title":"Manual Installation Requirements","text":"<p>Vue.js, Quasar, Python, NumPy, CalculiX, PostgreSQL, Flask</p>"},{"location":"WP06/htdt/#how-to-use","title":"How To Use","text":"<ul> <li>BI (Business Intelligence) Dashboard highlights critical KPIs of the thermoforming process.</li> <li>Configuration page set the dimension and the material of the working material, i.e. the sheet. Default material is HIPS.</li> <li>Cycle time, Heater Array scaling ratio can also be set on this menu.</li> <li>Switch between pre-heater and main-heater menus to monitor sensor data on top of prediction of the thermal profile of the sheet. </li> </ul>"},{"location":"WP06/htdt/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>A Walkthrough YouTube video will be added</p>"},{"location":"WP07/Interlinking-WP7/","title":"Interlinking","text":""},{"location":"WP07/Interlinking-WP7/#general-description","title":"General Description","text":"<p>The Interlinking subcomponent provides an alternative way to integrate ZDZW Assets and other components into the ZDZW Platform that could not be directly connected via a REST API and/or the message bus, eg because of proprietary protocols or missing service interfaces. This is achieved by implementing custom connectors that connect these sources to the Services API Management and/or the Message Bus via the Interlinking subcomponent.</p>"},{"location":"WP07/Interlinking-WP7/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>    HTTP(S) Proxying (APIGW)    </li> <li>    Web UI for the administration and configuration of platform interlinks </li> <li>    Interconnection and interoperability of multiple ZDZW instances running on different machines </li> <li>    Interlinking with other external (commercial) platforms </li> <li>    Interlinking to Marketplaces </li> </ol>"},{"location":"WP07/Interlinking-WP7/#architecture-diagram","title":"Architecture Diagram","text":"<p>The architecture diagram here displayed is the same as the Interoperability solution, since it is tightly coupled with it.</p> <p></p>"},{"location":"WP07/Interlinking-WP7/#image-overview","title":"Image Overview","text":"<p>The Inter-platform Interoperability component\u2019s aim is to link the ZDZW platform with other external platforms. This will, for example, include a layer to link data sources from different platforms and integrates security between the platforms to ensure appropriate access procedures across the platforms including connections to other instances of ZDZW. </p> <p> </p>"},{"location":"WP07/Interlinking-WP7/#hardware-components","title":"Hardware Components","text":"<p>None</p>"},{"location":"WP07/Interlinking-WP7/#computation-requirements","title":"Computation Requirements","text":"Requirement minimal recommended CPU 0.5 vCPU 1 vCPU RAM 256 MiB 512 MiB Storage 32 MiB 128 MiB"},{"location":"WP07/Interlinking-WP7/#installation-procedure","title":"Installation Procedure","text":"<p>This component will be provided for installation as Kubernetes Helm Chart. Option descriptions and further instructions will be provided here once the chart is finished.</p>"},{"location":"WP07/Interlinking-WP7/#how-to-use","title":"How To Use","text":"<p>The component provides a Web UI for configuration. The steps and functionalities vary by platform, but can be generalized as the following:</p> <ol> <li>Click the \"Create Interlink\" Button in the menu bar.</li> <li>Select the platform type.</li> <li>Enter a name for the interlink. This cannot be changed later.</li> <li>Confirm and switch to the newly created interlink by clicking on its name in the sidebar on the left.</li> <li>Enter the required information to configure the interlink and click save.</li> <li>Navigate to the interlinking functions using the tab menu.</li> </ol>"},{"location":"WP07/Interlinking-WP7/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>None</p>"},{"location":"WP07/Interoperability-WP7/","title":"INTEROPERABILITY","text":""},{"location":"WP07/Interoperability-WP7/#general-description","title":"General Description","text":"<p>The Interoperability solution is composed of various components to enable standard and secure communication and data transfer among ZDMP Solutions and with external systems. The components of the interoperability solution are the API Gateway, the Message Bus and the Complex Event Processing (CEP). The APIGW enables ZDZW Assets to use other ZDZW Assets in a standardized and secure way. This is achieved through manageable REST APIs provided by a Services API Management (APIGW) which exposes specific services offered by the connected ZDZW Assets. These APIs can be fully customized, eg to only expose certain functions or to restrict the API access to specific entities. This component also will be available as developer version for local purposes. The component interacts with the Message Bus, which is also part of the Interoperability Platform. The component provides ZDZW Assets with a message bus \u2013 a standardized communication interface to exchange messages, events, and data. This message bus implements a publish/subscribe messaging concept, which allows the connected ZDZW Assets to broadcast (publish) information on specific topics and to listen for certain events on these topics(subscribe) and to support real time analytics. The component interacts with the Message Bus component. It implements a concept, which allows the information send by any ZDZW assets to the message bus being analysed in real-time and thus creating higher level events out of low level information.</p>"},{"location":"WP07/Interoperability-WP7/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>    HTTP(S) Proxying (APIGW)    </li> <li>    Secure APIs with OpenID Connect (APIGW) </li> <li>    Request/Response transformation (APIGW) </li> <li>    Emulation of endpoints (APIGW)  </li> <li>    Register/deregister APIs (APIGW)    </li> <li>    API Management (MessageBus) </li> <li>    Message Bus (MessageBus)    </li> <li>    Streaming Analytics (MessageBus)    </li> <li>    Interlinking (MessageBus)   </li> <li>    Event Monitoring (CEP)  </li> <li>    Connectivity to several Message Bus topics (CEP)    </li> <li>    Graphical development tolos (CEP)   </li> <li>    Sophisticated analytics (CEP)   </li> </ol>"},{"location":"WP07/Interoperability-WP7/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"WP07/Interoperability-WP7/#image-overview","title":"Image Overview","text":"<p>These solution does not provide a User Interface, so no mock-ups are displayed. The ApiGW is a backend component, that allows ZDZW Platform and zApp developers to define and configure REST APIs to expose services provided by their ZDMP Assets. It is a component running centrally on the reference platform or locally for developer purposes.  The component is a developer tool running in the background that works locally without a specific user interface, based on Java Spring Web Flow. Instead, it can be used by directly executing API calls by using Postman or a similar tool make POST requests. This interface provides the API to interact with the Message Bus. It allows the connected components to broadcast information such as events or data by publishing messages on certain topics. Similarly, it enables the connected components to receive messages by subscribing to specific topics. The Message Bus of ZDZW will be based on RabbitMQ. Due to this reason, there will not be a huge demand for a specific ZDZW user interface as of now. This component listens for specific low-level events on the Message Bus such as sensor data to analyse and process them in real-time. The analysis results in the form of high-level events are subsequently published via the Message Bus.  It is also a backend component without specific user interface. Rules for real time can be programmed in a development environment and then deployed and used server side.</p>"},{"location":"WP07/Interoperability-WP7/#hardware-components","title":"Hardware Components","text":"<p>None</p>"},{"location":"WP07/Interoperability-WP7/#computation-requirements","title":"Computation Requirements","text":"Component min CPU rec CPU min RAM rec RAM min Storage rec Storage Message Bus 0.25 vCPU 1 vCPU 1 GiB 4 GiB 1 GiB 5 GiB API Gateway 0.25 vCPU 1 vCPU 256 MiB 512 MiB 32 MiB 128 MiB Complex Event Processing 2 vCPU 2 vCPU 4 GiB 4 GiB 2.5 GiB 3.5 GiB Total 2.5 vCPU 4 vCPU 5.25 GiB 8.5 GiB 3.5 GiB 8.6 GiB"},{"location":"WP07/Interoperability-WP7/#installation-procedure","title":"Installation Procedure","text":"<p>All components will be provided for installation as a Kubnetes Helm Chart. Option descriptions and further instructions will be provided here once the charts are finished.</p>"},{"location":"WP07/Interoperability-WP7/#how-to-use","title":"How To Use","text":""},{"location":"WP07/Interoperability-WP7/#message-bus","title":"Message Bus","text":"<p>The Message Bus component is based on the RabbitMQ open-source message broker. For instructions on how to use this software, please refer to the official documentation at https://www.rabbitmq.com/documentation.html.</p>"},{"location":"WP07/Interoperability-WP7/#api-gateway","title":"API Gateway","text":"<p>The API Gateway component is a HTTP(S) proxy server without a graphical user-interface. It can be configured using a RESTful HTTP API. The documentation of this API can be accessed via the running component at \\&lt;Gateway URL&gt;/swagger-ui.html . The main functions include: * Access management: OpenID Connect compatible authorization/authentication servers can be configured to restrict access to the gateway to valid tokens from these servers. By default, the gateway can be accessed using HTTP Basic authentication using the credentials admin:admin . * Route management: A route is a gateway endpoint that proxies requests to an external HTTP(S) API. Routes can only be accessed by using valid tokens configured in the access management interface. Also, filters can be added to routes, which allow for additional functionalities like authorization at the external API or setting headers in outgoing requests. Routes can also proxy traffic to other Kubernetes services that do not expose a public endpoint. * Global Filters: Global Filters can be applied to multiple routes at the same time and updated once for all affected routes, for example to configure external authorization for multiple routes that use the same credentials.</p>"},{"location":"WP07/Interoperability-WP7/#complex-event-processing-cep","title":"Complex Event Processing (CEP)","text":"<p>The Complex Event Processing component is based on the APAMA Community Edition, a freemium version of Apama that can be used to learn about, develop and put streaming analytics applications into production. For instructions on how to use this software, please refer to the official documentation at https://techcommunity.softwareag.com/en_en/products/iot---analytics/apama.html#apama-documentation</p>"},{"location":"WP07/Interoperability-WP7/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>RabbitMQ Documentation</p>"},{"location":"WP07/KubernetesPlatform/","title":"KubernetesPlatform","text":""},{"location":"WP07/KubernetesPlatform/#general-description","title":"General Description","text":"<p>This solution provides the key infrastructure where the ZDZW solutions are deployed and integrated. The Kubernetes Platform as per its descriptive name, is based on Kubernetes and Containers technologies.</p> <p>Containers have quickly become the standard for application deployment. Containers are units of software that package up code and all dependencies so that an application is able to run quickly and reliably in various computing environments, from OnPremise to OnCloud to Edge deployments.</p> <p>Kubernetes is the leading container orchestration technology and has become the de facto standard for container orchestration. Kubernetes is a portable, extensible, open-source platform for managing containerized workloads. It facilitates declarative configuration and automation, takes care of scaling and failing over applications, provides deployment patterns, and more. </p> <p>The Kubernetes Platform is a pre-packaged Kubernetes bundle, based on Kubernetes K3s, with several other tools to provide a fully featured production ready Kubernetes environment, such as:</p> <ul> <li>K3s: the lightweight Kubernetes, is a fully compliant Kubernetes distribution, easy to install, half the memory, all in a binary of less than 100 MB among other enhancements</li> <li>Rancher: Kubernetes cluster management tool with a user friendly UI</li> <li>Helm: the package manager for Kubernetes. The ZDZW solutions will be packaged using Helm charts</li> <li>Ingress Controller: the entry point for the cluster through the applications URLs using HTTP/HTTPS or TCP</li> <li>Service Mesh: transparent layer for enhanced connectivity, security, control and observability in the cluster</li> <li>Kubernetes Storage: provide the Kubernetes native storage for the stateful applications</li> </ul> <p>The Platform can be installed on-Cloud, on-Prem and on the Edge, providing great flexibility for the different ZDZW solutions needs. Within the scope of the project, there is the possibility to create a prototype of a Hardware box for Edge and on-Prem installation and connectivity with the cloud platform.</p>"},{"location":"WP07/KubernetesPlatform/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>Installation Process: The Platform provides a user friendly installation process for easy installation to non-experienced users.    In any case, management of the platform itself, will require some expertise.</li> <li>Flexibitlity of installation modes: supports different installation modes from on-cloud to on-prem and edge</li> <li>Development installation: The suite provides a development version of the platform so-called miniZDZW for ZDZW solutions developers to be able to test deployment of the apps in a development environment</li> <li>Platform Scalability: grows in resources as needed, i.e. add additional nodes</li> <li>Management Interface: provides a user friendly UI for cluster management. Management of the platform itself will require some expertise</li> <li>Management of Apps: provides the interface and API to manage apps in the platform : deployment, deletion, upgrade</li> <li>Monitoring of Platfom and Apps: provides the interface and API to monitor the platform itself and the apps running in the platform</li> <li>App Packaging \u2013 Helm Charts: provides the app packaging in the form of Helm charts for repeatable deployments. Provides platform side configuration to be applied during deployment of the apps</li> <li>Base Charts Engine: provides a cli to automatically create the application Helm chart based on base charts defined by operations or devops teams</li> <li>Networking-Connectivity: provides secure networking and connectivity among the installed apps and to and from outside the cluster</li> <li>Log management: ability to retrieve application logs for troubleshooting and debugging</li> <li>Native Storage: provide Kubernetes native storage for stateful applications</li> </ol>"},{"location":"WP07/KubernetesPlatform/#architecture-diagram","title":"Architecture Diagram","text":"<p>This Figure represents a high level architecture diagram of the ZDZW Kubernetes Platform. </p> <p>The ZDZW Platform solution has the following components:</p> <ul> <li> <p>Kubernetes: leading technology in container orchestration, key component that manages the deployment and integration of ZDZW applications. The current distribution used is K3s, which is a lightweight version of Kubernetes as described above. One of the main advantages of using the K3s distribution is that the same technology can be used to install a large OnCloud Kubernetes cluster, or a single node cluster deployed on the edge. The current Kubernetes version installed is 1.26 which is the most up to date version compatible with Rancher</p> </li> <li> <p>Management UI - API: open source software mainly based on Rancher that allows to deploy and manage Kubernetes clusters in a more user-friendly way both on-premise and on-cloud. The Rancher application will provide not only the management interface for platform administrators but also the app installation interface and API integration for ZDZW users. The current Rancher version being used is 2.7.9</p> </li> <li> <p>ServiceMesh \u2013 Ingress Controller: allows to add transparently a layer to provide the platform with enhanced connectivity, security, control and observability. It uses a sidecar container deployed along the application container to provide the service mesh features. This service mesh also allows for connectivity between distributed clusters, such as on-prem to on-cloud. The Service Mesh is not yet installed in the current ZDZW Platform, and nginx ingress controller version is being used, but will be installed in the next iteration of the platform</p> </li> <li> <p>Gateway - Ingress: provides the entry point to the cluster and the service mesh for all network traffic. It exposes the applications through the internet. The current ingress resource is based on nginx ingress controller, and will likely evolve to ServiceMesh Gateway when the ServiceMesh is installed </p> </li> <li> <p>Monitoring UI-API: open source software mainly based on Prometheus + Grafana, provides the monitoring interface for the cluster and the ZDZW applications. It may provide also an interaction with the ZDZW usage traceability. Prometheus and Grafana have not beeen installed yet in the platform and the Rancher monitoring capabilities are being used</p> </li> <li> <p>Storage Management: provides the interface between Kubernetes and the physical storage, i.e. NFS controller, EBS controller \u2026 At the moment, the current setup is using local storage for apps but this will change in future iterations of the platform where NFS and/or Kubernetes native storage will be used </p> </li> <li> <p>CertManager: provides management of SSL certificates for secure connectivity ie. HTTPS, with verified signed certificates using Let\u2019s Encrypt. Cert-manager version is being used. The current certificates are self-signed since in this iteration no application is being exposed to internet. This will change in the next iteration of the platform, where Let's Encrypt signed certificates will be created for the applications</p> </li> </ul>"},{"location":"WP07/KubernetesPlatform/#image-overview","title":"Image Overview","text":"<p>Add here real screen shots from the apps or mockups from the Software Specification document</p>"},{"location":"WP07/KubernetesPlatform/#hardware-components","title":"Hardware Components","text":"<p>The current ZDZW Cloud platform is runing in Ascens cloud vendor using three Ubuntu 20 VMs with the requirements detailed in the next section. The Hardware details in this case are not required since provided by cloud vendor.</p>"},{"location":"WP07/KubernetesPlatform/#computation-requirements","title":"Computation Requirements","text":"<p>For the development installation of the platform or edge deployment (minizdzw), these are the minimum requirements. This can be increased depending on the number of ZDZW solutions to be deployed:</p> <ul> <li>4 CPUs</li> <li>6-8 GB RAM</li> <li>32 GB Storage</li> </ul> <p>For a private OnCloud/OnPrem deployment of the platform a minimum of three nodes cluster is recomended with requirements similar to the ones in the current ZDZW Cloud Platform:</p> <ol> <li> <p>Master Node</p> <ul> <li>8 CPUs</li> <li>8 GB RAM</li> <li>64 GB Storage</li> </ul> </li> <li> <p>Two Worker Nodes each with</p> <ul> <li>8 CPUs</li> <li>16 GB RAM</li> <li>256 GB Storage</li> </ul> </li> </ol>"},{"location":"WP07/KubernetesPlatform/#installation-procedure","title":"Installation Procedure","text":"<p>There are two different setups, one for development or edge deployment named after miniZDZW which is just a single node cluster version of the actual platform, and fully featured Kubernetes Platform.</p>"},{"location":"WP07/KubernetesPlatform/#minizdzw","title":"miniZDZW","text":"This is a tool to install a single node cluster version of ZDZW Kubernetes Platform. The user just needs to run a single script and this will install and configure all the required tools.<ol> <li>Clone the repository <code>bash git clone https://minizdzw</code></li> <li>Navigate to minizdzw folder <code>bash cd minizdzw</code></li> <li> <p>Run minizdzw <code>bash ./minizdzw.sh ip iface</code> where ip is the ip of the node where running the script and iface is the network interface of the ip. The script will install and configure all the required tools for miniZDZW platform to run. That is:</p> <ul> <li>Docker</li> <li>K3s (Kubernetes)</li> <li>Helm</li> <li>Cert-manager</li> <li>Rancher</li> <li>Creates a self signed certificate to use by the ingress controller</li> <li>Nginx Ingress Controller</li> <li>nginx docker (load balancer)</li> </ul> </li> <li> <p>Once the script has successfully installed miniZDZW, access the Rancher URL at https://rancher.minizdzw.$nodeip.sslip.io, which is the default domain used during the installation. Enter the default admin password set by default to \"admin\" and create a new admin password. Start managing the Kubernetes cluster and apps using the Rancher UI. In the \"How To Use\" section there will be detailed instructions on how to use and perform the key management actions of the Kubernetes platform</p> </li> </ol>"},{"location":"WP07/KubernetesPlatform/#kubernetes-platform-installation-kit","title":"Kubernetes Platform Installation Kit","text":"<p>An installer for the Kubernetes Platform called Installation Kit has been developed.   The idea is to allow non experienced users to deploy the platform along with a fully featured Kubernetes cluster, with all tools installed and configured.   The ZDZW Installation Kit will run as a docker container on an installation node or even on a local laptop and will install the platform in the remote nodes.   The Installation Kit allows the user to configure and customize the installation process using some forms via browser and will run Ansible playbooks on the backend in order to effectively install the platform.</p> <p>The main steps to install the Kubernetes Platform are:</p> <ol> <li> <p>Run the Installation Kit image</p> <p><code>bash sudo docker run</code></p> </li> <li> <p>Navigate to your local browser at http://127.0.0.1</p> </li> <li>Enter the user to be used for installation, by default icedrancher</li> <li>Download the create_user.sh script and run it manually on the nodes where the platform will be installed. This script will create the installation user in each platform node and distribute the ssh keys in order to be able to run the commands from the installation node. Clik the \"Next\" button once the script has been run in all nodes</li> <li>Fill in all the forms with the required cluster info<ul> <li>1 Basic Options: enter the cluster name</li> <li>2 Load Balancer Options: enable an external nginx load balancer option if you don't have a load balancer. Enter the ip of the node where the nginx loadbalancer will be installed.</li> <li>3 K3s Options: enter the details of the nodes where the cluster will be installed.    Click on add in order to add a new node. In the pop up dialogue enter the Node Type (master or worker), the IP and the network interface. You have to create at least one master node and an odd number of them.</li> <li>4 Rancher options: select whether to use the same domain for Rancher than the applications one</li> <li>5 Confirmation: review the options selected and click on the Install button. The installation will start. Once installation is successfull click on Done button and the cluster is installed. You can now navigate to the Rancher UI.</li> </ul> </li> </ol>"},{"location":"WP07/KubernetesPlatform/#how-to-use","title":"How To Use","text":"<p>This section provides the steps not only on how to use the Kubernetes Platform and the Rancher UI but also instructions on how to prepare the Helm Charts for the platform.</p>"},{"location":"WP07/KubernetesPlatform/#kubernetes-platform","title":"Kubernetes Platform","text":"<p>Once the ZDZW Kubernetes Platform has been installed and the Rancher UI can be accessed as show in section \"Installation Procedure\", these key management actions can be performed:</p> <ul> <li> <p>Inspect the cluster and check number of nodes, resources, etc   Ref to snapshot</p> </li> <li> <p>A user can register a catalog or repository in order to be able to deploy apps from that catalog. A catalog is just a repository, git or helm, where helm charts are stored. Rancher has already some default catalogs registered. In order to create the new one, navigate to the cluster, apps, repositories, and click on the \"Add Catalog\" button. Fill in the form with the credentials for a private repository and click \"Create\". The Catalog is added to Rancher and the apps will be displayed in the Apps view   Ref to snapshot</p> </li> <li> <p>Applications can be deployed from the registered Catalog. Navigate to the Apps section where the list of deployed apps is displayed. Click on Launch button. The list of available apps is displayed. Select the app to be deployed, fill in the configuration form and click on Install. The application will be deployed to the platform.   Ref to snapshot</p> </li> <li> <p>Inspect the application and logs. </p> </li> <li> <p>Delete applications</p> </li> </ul> <p>For more detailed description on these steps, refer to Additional Learning Materials, the Rancher documentation.</p>"},{"location":"WP07/KubernetesPlatform/#helm-charts","title":"Helm Charts","text":"<p>Helm is the package manager for Kubernetes. It helps you manage Kubernetes applications by combining the different Kubernetes resources an application is composed of in a single package, the Helm Chart, that you can create, share, version, publish, install, upgrade and delete. It also provides templating functionality so that different values can be configured for an application.</p> <p>But creating a Helm Chart and the Kubernetes resources is not an easy task since it requires some Kubernetes knowledge. For that reason the Kubernetes Platform provides a base or template Helm Chart to be imported as a subchart in an application chart.</p> <p>The idea is that developers creating zwApps can re-use this base chart as many times as per the number of modules in their zwApp, i.e. frontend, backend, without any knowledge of Kubernetes related stuff, and only having to override a set of values.</p> <p>The base chart includes the following resources pre-configured to be re-used out of the box:</p> <ul> <li>A deployment</li> <li>A service</li> <li>An ingress</li> <li>A persistent volume and persistent volume claim</li> <li>A service account</li> <li>A registry secret</li> <li>A pre-configured set of values in values.yaml</li> </ul> <p>With this, developers just need to include the base chart in their chart folder, and override some values depending on apps needs, that is just having to update a single yaml file vs creating all Kubernetes related resources yaml files, thus simplifying developers integration with Kubernetes and the Kubernetes learning curve.</p> <p>These are the detailed step by step instructions in order to use the base Helm Chart. We will refer to the base chart as zapp-base and to the zwapp chart as zwapp:</p> <ul> <li> <p>Create a helm chart using helm cli  <code>helm create zwapp</code> where zwapp is the name of the chart to be created</p> </li> <li> <p>Remove non-required folders and files <code>rm -rf zwapp/templates/* rm zwapp/values.yaml</code></p> </li> <li> <p>Copy the base helm chart to the zwapp charts subfolder with a different name i.e. zmodule1, and replace the Chart.yaml name accordingly. Copy the base helm chart values.yaml to the zwapp values <code>cp -r zapp-base/ zwapp/charts/zmodule1 sed -i 's/zapp-base/zmodule1/' zwapp/charts/zmodule1/Chart.yaml cp zwapp/charts/zmodule1/values.yaml zwapp/</code></p> </li> <li> <p>Edit the zwapp values in order to leave only the variables you need to override. Add the reference to the subchart name (the base chart) and ident all related values to the right. This is an example of a values file in order to override the app related values <code>zmodule1:   app:     name: myname     port: 80</code></p> </li> <li> <p>Add an optional configmap if you need to include zwapp related env variables to be set. For this, you can set the values app.env to true (default). Then add in the zwapp/templates folder a configmap with the name $appname-configmap.yaml, where $appname is the name of the app in the values variable app.name. This is an example of a configmap <code>apiVersion: v1 kind: ConfigMap metadata:   name: {{ .Values.zmodule1.app.name }}-configmap   namespace: {{ .Release.Namespace }} data:   WELCOME_MESSAGE: {{ .Values.welcome_message }}</code></p> </li> </ul> <p>For additional instructions and examples see ZDZW Integration and Helm Chart links at Additional Learning Materials section.</p>"},{"location":"WP07/KubernetesPlatform/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Kubernetes https://v1-26.docs.kubernetes.io/docs/home/</p> <p>Helm https://helm.sh/</p> <p>K3s https://k3s.io/</p> <p>Rancher https://ranchermanager.docs.rancher.com/v2.7/getting-started/quick-start-guides</p> <p>Integration https://github.com/zdzw-eu/integration/tree/main</p>"},{"location":"WP07/Marketplace/","title":"Marketplace","text":""},{"location":"WP07/Marketplace/#general-description","title":"General Description","text":"<p>The Marketplace is a central repository for manufacturing sector applications, allowing users to search, purchase, and license applications. It provides a collaborative system for users to make requests, rate, and review zApps. Developers can access licenses for testing their own products. The backend offers content management, metadata organization, license management, and affiliate marketing. This is an App for: Manufacturers: Manufacturers benefit from the Marketplace by easily finding and obtaining licenses for specialized applications tailored to their specific needs in the manufacturing sector. They can search for applications based on various characteristics such as category, price, and payment type, streamlining the process of deploying applications across their infrastructure. Developers: The Marketplace provides developers with a centralized platform to showcase and distribute their applications. They can easily manage and update metadata, organize different licenses, and set pricing for their products through the backend content management system. This streamlines the process of reaching a wider user base and simplifies licensing management.</p>"},{"location":"WP07/Marketplace/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ul> <li>Intuitive Application Discovery: The marketplace offers a user-friendly interface that simplifies the process of discovering specialized applications (zApps) for manufacturers. Users can easily explore and find relevant solutions based on categories, pricing, and licensing models.</li> <li>User Feedback and Ratings: Manufacturers can benefit from the feedback and ratings provided by other users. This feature allows them to make informed decisions by considering the experiences and opinions of their peers.</li> <li>Flexible Licensing Models: The marketplace supports a variety of licensing models to cater to different manufacturing needs. From one-time fees to advanced subscription options like pay-as-you-go and pay-per-use, manufacturers have the flexibility to choose the most suitable licensing model for their requirements.</li> <li>Advanced Subscription Options: In addition to traditional licensing models, the marketplace introduces innovative subscription-based options. Manufacturers can opt for pay-per-zero-waste saved or other customized subscription plans that align with their sustainability goals and usage patterns.</li> <li>Transparent Cost Visualization: The marketplace provides manufacturers with visibility into ongoing costs per application. This feature allows them to track and manage their expenses effectively, making budgeting and resource allocation more efficient.</li> <li>Secure Payment Gateway Integration: Seamless integration with trusted third-party secure payment gateways ensures secure and convenient payment transactions. Manufacturers can make payments for the acquired applications with confidence, knowing that their financial information is protected.</li> <li>Seamless Interlinking with ZDZW Traceability: The marketplace seamlessly integrates with the ZDZW secure usage traceability for monetization service. This feature guarantees reliable and transparent tracking of usage data, enabling manufacturers to make data-driven decisions and optimize resource allocation based on accurate insights.</li> <li>Robust Application Management: The marketplace offers comprehensive application management capabilities. Manufacturers can easily access and manage their purchased zApps, including license renewal, deployment requests, and updates.</li> <li>Customized Categories and Search: Manufacturers can further refine their search for applications by utilizing customized categories set up in the marketplace. This feature allows them to narrow down their options and find the most relevant solutions for their specific needs.</li> <li>Streamlined User Experience: The marketplace prioritizes user experience by providing a seamless and efficient interface. With streamlined processes, intuitive navigation, and responsive design, manufacturers can easily navigate the marketplace, explore applications, and carry out transactions without unnecessary friction</li> </ul>"},{"location":"WP07/Marketplace/#architecture-diagram","title":"Architecture Diagram","text":""},{"location":"WP07/Marketplace/#image-overview","title":"Image Overview","text":""},{"location":"WP07/Marketplace/#marketplace_1","title":"Marketplace","text":"<ul> <li>Marketplace Home </li> <li>Marketplace Purchases </li> </ul>"},{"location":"WP07/Marketplace/#product-license-manager","title":"Product License Manager","text":"<ul> <li>Admin Products </li> <li>PLM API </li> <li>Costs Mockup Dashboard </li> </ul>"},{"location":"WP07/Marketplace/#hardware-components","title":"Hardware Components","text":"<p>The code will run on a basic laptop, with docker.</p>"},{"location":"WP07/Marketplace/#computation-requirements","title":"Computation Requirements","text":"<p>Minimum 1 VCPU, 1 Gb RAM, 5Gb storage Recommended 4 VCPU, 4 Gb RAM, 80 Gb Storage Docker version 19 minimum</p>"},{"location":"WP07/Marketplace/#installation-procedure","title":"Installation Procedure","text":"<p>Step by step on how to install the application:</p> <p>Git clone this repository: SSH:</p> <pre><code>git clone git@github.com:zdzw-eu/Marketplace.git\n</code></pre> <p>HTTPS:</p> <pre><code>git clone https://github.com/zdzw-eu/Marketplace.git\n</code></pre> <ul> <li>Standalone using docker compose Navigate to the orchestration folder using cmd or similar.</li> </ul> <pre><code>cd orchestration\n</code></pre> <p>Execute the command:</p> <pre><code>docker compose up\n</code></pre> <p>The Marketplace front end will be available at: http://localhost:8002/ The Product license manager will be available at: http://localhost:8000/</p> <p>If you are a developer and want to edit some of the code and see your changes in real time, here is how you can do that. Both the PLM and the Marketplace containers work in a way that it's looking at the directory and re-building automatically whenever you update the code.  If you want to run the Marketplace on a different port for example, you navigate to:</p> <pre><code>cd subsystems\\marketplace\n</code></pre> <p>You can then run the marketplace using:</p> <pre><code>ng serve --port 4401\n</code></pre> <p>As an example if you want to run it on port 4401</p> <p>Each system also has it's own environment variables and these are placed inside of a .env file inside of each subsystem/software's name The Marketplace has the following environment variables and here is an example of them in a .env file:</p> <pre><code>PORTAL_FRONTEND_URL=http://localhost:4200\nPORTAL_API_URL=http://localhost:5100\nKEYCLOAK_URL=http://keycloak:8080/auth\nKEYCLOAK_CLIENT_ID=ZDMP_Portal\nPRODUCT_LICENSE_MANAGER_API=http://localhost:8000/api\nCART_MEDIATOR_URL=http://localhost:8081\nPRODUCT_LICENSE_MANAGER_URL=http://localhost:8000\n</code></pre> <p>Cart mediator has the following .env env variables:</p> <pre><code>ZDMP_MARKETPLACE_UI_URL=http://localhost:8082/\nZDMP_PRODUCT_LICENSE_MANAGER_URL=http://localhost:8000/\nZDMP_CART_MEDIATOR_URL=http://localhost:8081/\nZDMP_CART_MEDIATOR_JWT_KEY=de2d3f3892ecf7000fe8cddb2ce0c801217ec0f31079ba56beff62f20a0b982f\nZDMP_CART_MEDIATOR_ORDER_SECRET=ozsl6h662Jf5r+1rGU44sg==\nZDMP_CART_MEDIATOR_MONGODB_CONNECTION_STRING=mongodb://admin:admin@host.docker.internal:27018/?authSource=admin&amp;readPreference=primary&amp;ssl=false\nZDMP_CART_MEDIATOR_MONGODB_DATABASE=zdmpCart\nZDMP_KEYCLOAK_URL=http://host.docker.internal:8080/\nZDMP_KEYCLOAK_PLM_CLIENT_ID=ZDMP_Portal\nZDMP_KEYCLOAK_PLM_CLIENT_USERNAME=xxx\nZDMP_KEYCLOAK_PLM_CLIENT_PASSWORD=xxx\nZDMP_KEYCLOAK_PLM_CLIENT_REALM=i4fs\nZDMP_CART_MEDIATOR_MAIL_HOST=xxx\nZDMP_CART_MEDIATOR_MAIL_PORT=xxx\nZDMP_CART_MEDIATOR_MAIL_ENCRYPTION=xxx\nZDMP_CART_MEDIATOR_MAIL_USERNAME=xxx\nZDMP_CART_MEDIATOR_MAIL_PASSWORD=xxx\nZDMP_CART_MEDIATOR_FROM=xxx\nZDMP_CART_MEDIATOR_STRIPE_KEY=xxx\nZDMP_CART_MEDIATOR_STRIPE_WEBHOOK_ENDPOINT_SECRET=xxx\nENVIRONMENT=development\n</code></pre> <p>Replacing the xxx's with values for Stripe and also a mail server.</p> <p>Finally the Product license manager has the following .env file variables:</p> <pre><code>APP_NAME=ZDMP\nAPP_ENV=local\nAPP_KEY=base64:O+AXgu3+EAXTs7npuOkmvBWbn4lpbsPYXaV2DDRAz18=\nAPP_DEBUG=true\nAPP_URL=localhost:8000\nQUERY_DETECTOR_ENABLED=false\n\nKEYCLOAK_BASE_URL=http://host.docker.internal:8080/auth\n\nKEYCLOAK_REALM=i4fs\nKEYCLOAK_CLIENT_ID=admin-cli\nKEYCLOAK_REALM_PUBLIC_KEY=2caf5527-892f-4ad3-a5d7-420155762130\nMASTER_KEYCLOAK_REALM=master\nMASTER_KEYCLOAK_CLIENT_ID=admin-cli\nMASTER_KEYCLOAK_USER=admin\nMASTER_KEYCLOAK_PASSWORD=Pa55w0rd\n\nDASH_BUTTON_LOGIN=http://localhost:4200/auth/login\nPORTAL_URL=http://localhost:5100\n\nKEYCLOAK_PRODUCT_OWNER_ROLE=ZDMP_Marketplace_Product_Owner\n#KEYCLOAK_MODERATOR_ROLE=ZDMP_Marketplace_Moderator\nKEYCLOAK_ADMIN_ROLE=ZDMP_Marketplace_i4FS_admin\n\nSECURE_INSTALLATION_URL=https://secure-installation-api-zdmp.platform.zdmp.eu/zdmp/api/v_0_1\nMONITORING_AND_ALERTING_URL=https://monitoring-and-alerting-api-zdmp.platform.zdmp.eu/\n\nMARKETPLACE_URL=http://host.docker.internal:8082/\n\nLOG_CHANNEL=daily\n\nDB_CONNECTION=mysql\nDB_HOST=mysql-db\nDB_PORT=3306\nDB_DATABASE=db\nDB_USERNAME=dbuser\nDB_PASSWORD=secret\n\nBROADCAST_DRIVER=log\nCACHE_DRIVER=file\nQUEUE_CONNECTION=database\nSESSION_DRIVER=file\nSESSION_LIFETIME=120\n\nREDIS_HOST=127.0.0.1\nREDIS_PASSWORD=null\nREDIS_PORT=6379\n\n#MAIL_MAILER=smtp\nMAIL_MAILER=xxx\nMAIL_HOST=xxx\nMAIL_PORT=2525\nMAIL_USERNAME=xxx\nMAIL_PASSWORD=xxx\nMAIL_ENCRYPTION=tls\nMAIL_FROM_ADDRESS=noreply@zdmp.com\n\nAWS_ACCESS_KEY_ID=minio\nAWS_SECRET_ACCESS_KEY=minio123\nAWS_DEFAULT_REGION=us-east-1\nAWS_BUCKET=test\nMINIO_ENDPOINT=xxx\n\nPUSHER_APP_ID=\nPUSHER_APP_KEY=\nPUSHER_APP_SECRET=\nPUSHER_APP_CLUSTER=mt1\n\nMIX_PUSHER_APP_KEY=\"${PUSHER_APP_KEY}\"\nMIX_PUSHER_APP_CLUSTER=\"${PUSHER_APP_CLUSTER}\"\nFILESYSTEM_CLOUD=minio\n#MEDIA_DISK=minio\nMEDIA_DISK=local\n</code></pre> <p>Some further help can be found inside of the readme file's of each subsystem also.</p> <ul> <li>In the Kubernetes platformm using helm charts: description of the different options to do.</li> </ul>"},{"location":"WP07/Marketplace/#how-to-use","title":"How To Use","text":""},{"location":"WP07/Marketplace/#marketplace_2","title":"Marketplace","text":""},{"location":"WP07/Marketplace/#marketplace-user-interface-and-payment-system","title":"Marketplace User Interface and Payment System","text":"<p>The frontend for users focuses on findability of the created zApps. For this, the following tools are implemented: * A sorter allows the sorting of the shown list by Category, Price, Name or Rating. * The categories on the left-hand menu let the user quickly filter by Category. The discover mode lets the user search for a flat list of all categories of zApps. These categories are obtained from the Product License Manager and can be configured by the administrator * The search bar allows filtering the results shown by title, dynamically updating as necessary. It also includes search suggestions based on matching product titles * The tags allows filtering based on a number of tags, this is similar to the categories but more diverse. </p>"},{"location":"WP07/Marketplace/#product-details","title":"Product Details","text":"<p>The single items are now comprised of a title, version information and details, links to any documentation/information, categories, screenshots, a screenshot gallery, a description, a list of dependencies, a ratings widget displaying the aggregated ratings for the product and a paginated list of reviews.  This detail page also shows combinations of license and currencies in which the zApp is available. * Product View Details Page </p>"},{"location":"WP07/Marketplace/#product-licenses","title":"Product licenses","text":"<p>If the user clicks the \u201cBuy\u201d button, they will be presented with licenses including their durations. This list only includes the licenses which are available for the currently selected currency. After selecting the desired license, the zApp will be added to the cart. If the zApp is not available in the current currency, the buy button is disabled. If the current user of the Marketplace User Interface is the developer of the opened product, the detail screen also provides a \u201cDeveloper Access\u201d functionality. If the user clicks on this button, they will be presented with the same license selection dialog as if they would try adding the product to the cart. The difference is that upon selecting a license, the developer obtains the license for their product directly. This allows the developer to test their product without having to buy their product. * Product License Selection </p>"},{"location":"WP07/Marketplace/#cart","title":"Cart","text":"<p>After adding the desired zApps to the cart, the user can use the \u201cGo to checkout\u201d button to initiate the payment process. The payment process in place currently puts the software items in the shopping cart with the selected license. The next step is gathering or confirming the shipment address as this is necessary for tax accounting. After sending the order, the user will be redirected to the integrated payment provider Stripe, where they will enter their payment details. After entering the payment details was successful, the user is redirected back to the order confirmation screen. In the background, the payment provider processes the payment and notifies the payment backend of the Marketplace if it was successful or not. If the payment was valid, the payment backend notifies the Product License Manager to store new licenses according to the placed order. In the final step, the summary containing the ordered licenses is displayed in a result screen. After purchasing the licenses, there is a purchases menu item on the lower left-hand side. It leads to a page displaying all currently confirmed licenses for the user\u2019s account. This may not yet contain the ordered licenses, as this depends on the payment provider confirming the payment. * Cart  </p>"},{"location":"WP07/Marketplace/#product-license-manager_1","title":"Product License Manager","text":""},{"location":"WP07/Marketplace/#dashboard","title":"Dashboard","text":"<p>The dashboard contains various widget which displays helpful information based on user roles(developer/admin).</p> <p>At the top of the dashboard are three counters: One for the products, one for the licenses, and the last for the affiliates. The next widget is a chart that displays the license selling evolution in time and monthly and yearly totals. The map widget displays the spreading across the world of the users that bought the licenses(see images above).</p> <p>Products list This screen contains the product list with some of its information such as ID, name, languages, etc. Each product has a status based on its current situation. The list has some additional buttons which allows the user to export that list in CSV, Excel, and PDF file. Also, it has two other buttons that allow the user to copy to clipboard the list or to print it. * Admin Dashboard  * User Dashboard </p>"},{"location":"WP07/Marketplace/#add-a-new-product","title":"Add a new product","text":"<p>Before adding a product, the developer should read and agree the developer agreement, then select a type of product (zApp, zComponent, other) and then start to complete the product information.</p> <p>Adding a new product is a process that can be performed in the following ways: Using the Product Manager UI or using the Product Manager REST API.</p> <p>Every time a product is created, it receives the \u201cReady for review\u201d status and an email is sent to designated moderators to check if the product has the right information. While the product is reviewed, it gets the status \u201cReviewing\u201d so it cannot be processed by another reviewer at the same time. After reviewing, the moderator sets the next status, either they will accept and product or they will reject it and send it back to the owner to do the necessary changes. Only the accepted products can be displayed in the Marketplace frontend.</p> <p>Within the UI can be found five tabs, each has a set of fields with a common purpose. The Info tab contains the main product information such: Product id, name, version, category, download file, dependencies, associated hardware/services etc. The primary language is English, and the main currency is Euro.</p> <p>The Licensing tab adds the possibility to add different licenses based on the duration, license type, price and currency, each license has license link and description.</p> <p>The Translations tab adds the possibility to translate the product name and product description. There is no limit, it can be translated to any known language.</p> <p>The images tab adds the possibility to display screenshots of the product and its functions</p> <p>The Asset Links tab allows users to add different links to their product which are then displayed on to the frontend for the user to view, these could be things such as videos, licensing links and more information for example.</p> <ul> <li>User Products </li> <li>User Adding new product  </li> </ul>"},{"location":"WP07/Marketplace/#categories","title":"Categories","text":"<p>Admins can edit the categories from this tab. These are the categories that can be selected when a user is adding their new product, or editing their existing one. It also changes how the Marketplace displays the products and categories. * Admin Categories </p>"},{"location":"WP07/Marketplace/#license-list","title":"License list","text":"<p>The license list can be seen via the Product License Manager UI. After each new license is purchased from the Marketplace that license is added to this table (see image above) and the owner is notified via email.</p>"},{"location":"WP07/Marketplace/#admin-settings","title":"Admin Settings","text":""},{"location":"WP07/Marketplace/#supported-platforms","title":"Supported Platforms","text":"<p>Each product is created for certain platforms. The product form has a select box with multiple options from which the user can choose the supported platforms. That list is manageable in the Supported Platforms screen where they can be added or edited.</p>"},{"location":"WP07/Marketplace/#license-types","title":"License Types","text":"<p>Each license has a type which is required for the notification system and the license management system. These types are manageable in the License Type screen</p>"},{"location":"WP07/Marketplace/#currencies","title":"Currencies","text":"<p>Each product has at least a price with a currency. This screen allows the user to edit the currency names and symbols at this point.</p>"},{"location":"WP07/Marketplace/#file-formats","title":"File Formats","text":"<p>This page allows the admin user to add supported file types to be uploaded during the adding of a new product. It displays how many files of each type currently exists in the system also. * Admin Payment Settings </p>"},{"location":"WP07/Marketplace/#user-bank-account","title":"User Bank Account","text":"<p>This page set's up the connection between the Product License Manager with a Stripe account. Once this connection is in place any payments made for products and sent to this Stripe account. * User Bank Account </p>"},{"location":"WP07/Marketplace/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"},{"location":"WP07/Portal/","title":"Portal","text":""},{"location":"WP07/Portal/#general-description","title":"General Description","text":"<p>The Portal is the main website entry point to the ZDZW platform and its applications and it is based on the ZDMP Portal. It displays the applications that can be accessed as icons and users can navigate to them through the portal. </p> <p>The Portal will allow for organisation and users registration providing the single sign on functionality and incorporating the security aspect of the ZDZW platform using Keycloak and its users and RBAC functionality. </p> <p>The Portal also includes the Dash Button, a web component that needs to be integrated in all ZDZW solutions to provide an interface to the portal for session and JWT token management and allow users to navigate between ZDZW applications and provide support to the single sign on.</p>"},{"location":"WP07/Portal/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>UI main platform entry point: provides a UI that is the main entry point to the platform</li> <li>Display list of available apps: displays the list available apps to be able to navigate from the portal to a ZDZW app</li> <li>Display information on available apps: displays the status of the available apps as installed or not installed, and a link to the management UI in order to install the application if not installed</li> <li>Register an organisation and admin user: allows for organisation registration and admin user of the organisation in keycloak</li> <li>User management: allows to create additional users for an organisation</li> <li>Role Management: allows to create and assign roles to users</li> <li>Dash Button: provides a web component for zdzw security integration and easy navigation between zdzw applications</li> <li>JWT Management: provides a web component for token management</li> <li>Security Realm: provides management of security realms</li> </ol>"},{"location":"WP07/Portal/#architecture-diagram","title":"Architecture Diagram","text":"<p>This Figure represents a high level architecture diagram of the ZDZW Portal </p> <p>The Portal solution has the following components:</p> <ul> <li> <p>Portal UI: this is the main ui to allow access to the platform and the different ZDZW applications, providing single sign on and the list of available apps</p> </li> <li> <p>Portal Backend: the backend component of the portal, being the responsible for getting the list of apps and status from the Platform, and also act as the interface with the security component Keycloak</p> </li> <li> <p>Portal API: the API of the Portal Backend to allow communication from the Portal UI and the Dash Button  </p> </li> <li> <p>Dash Button: web component integrated into ZDZW UI apps that allow communication with the Portal and the security component </p> </li> <li> <p>Keycloak: open source software for security management</p> </li> </ul>"},{"location":"WP07/Portal/#image-overview","title":"Image Overview","text":"<p>Add here real screen shots from the apps or mockups from the Software Specification document</p>"},{"location":"WP07/Portal/#hardware-components","title":"Hardware Components","text":"<p>The Portal does not have any hardware component.</p>"},{"location":"WP07/Portal/#computation-requirements","title":"Computation Requirements","text":"<p>The deployment of the Portal on top of ZDZW Platform, these are the minimum requirements:</p> <ul> <li> <p>1 CPU</p> </li> <li> <p>1 GB RAM</p> </li> <li> <p>2 GB Storage</p> </li> </ul> <p>These can be increased depending on the number of apps, users and organisations.</p>"},{"location":"WP07/Portal/#installation-procedure","title":"Installation Procedure","text":"<p>This section describes how to deploy the Portal on the ZDZW Kubernetes Platform and/or miniZDZW. It also describes how to integrate the Dash Button in a zwApp.</p>"},{"location":"WP07/Portal/#portal_1","title":"Portal","text":"<p>The Portal can be installed on the ZDZW Kubernetes Platform or miniZDZW with the helm charts and the Rancher UI as a regular zwApp, once the zdzwcatalog repository has been registered as per the ZDZW Kubernetes Platform instructions and following the instructions on how to install an app.</p> <p>The following variables can be set when installing the app:</p> <ol> <li> <p>Variable</p> </li> <li> <p>Variable</p> </li> </ol>"},{"location":"WP07/Portal/#dash-button","title":"Dash Button","text":"<p>The Dash Button is a Portal webcomponent that needs to be integrated by zwApps and will provide functionalities related to single-sign-on and navigation through the zwApps in the platform.</p>"},{"location":"WP07/Portal/#how-to-use","title":"How To Use","text":""},{"location":"WP07/Portal/#additional-learning-materials","title":"Additional Learning Materials","text":""},{"location":"WP07/UsageTraceability/","title":"Usage Traceability","text":""},{"location":"WP07/UsageTraceability/#1-general-description","title":"1. General Description","text":"<p>The Usage Traceability represents the central communication layer between the Marketplace, the Assets (applications users) and the ZDZW Blockchain. It provides the following main functions: *   Manages the smart contracts to be able to trace the usability of a ZDZW Inspection Solution *   Implements business logic to allow the usage data traceability and monetization management *   Stores in the ZDZW Blockchain the terms of the smart contract. *   Triggers payments according to the conditions implemented in the smart contract *   Tracks and store the usage of ZDZW applications by end-users in the Blockchain (BaaS)</p>"},{"location":"WP07/UsageTraceability/#2-top-ten-functionalities","title":"2. Top Ten Functionalities","text":"<ol> <li> <p>Establish a secure &amp; distributed database: The solution will store information regarding the acquisition and use of the zApps by different users in a distributed and secure manner. DLT-based technologies will be exploited for this purpose.</p> </li> <li> <p>Blockchain network management, deployment: The solution shall deploy and manage the chosen Blockchain platform.</p> </li> <li> <p>Blockchain integration with zApps and Marketplace: The solution will enable the Traceability Gateway to seamlessly connect the Marketplace's zApps acquisition processes with the Blockchain. This integration ensures secure storage and retrieval of usage data, creating a cohesive link between the acquisition of zApps and their traceability on the Blockchain.</p> </li> <li> <p>Integration flow runtime: The solution will enable a streamlined and dynamic integration flow runtime within the Traceability Gateway, allowing developers to efficiently manage data ingestion, processing, and blockchain registration.</p> </li> <li> <p>Data ingestion, data transformation, and blockchain registration functions: The solution will empower efficient data ingestion, seamless data transformation, and reliable blockchain registration functions within the Traceability Gateway, ensuring a cohesive and effective integration process.</p> </li> <li> <p>Function palette management: The solution will facilitate comprehensive function palette management within the Traceability Gateway, offering developers a versatile toolkit to customize and optimize integration flows for zApps traceability.</p> </li> <li> <p>Integration flow management: The solution will enable effective integration flow management within the Traceability Gateway, allowing developers to efficiently organize and oversee the flow of data for seamless zApps traceability.</p> </li> <li> <p>Integration flow edition user interface: The solution will provide an intuitive user interface for editing integration flows within the Traceability Gateway, ensuring a user-friendly experience for developers to customize and optimize the flow of data for zApps traceability.</p> </li> <li> <p>Function palette management UI: The solution will feature a user-friendly interface for managing the function palette within the Traceability Gateway, offering developers a seamless experience in customizing and optimizing integration flows for zApps traceability.</p> </li> <li> <p>Provide Smart Contract Templates: The solution shall provide templates for the smart contracts that will manage the storage of zApps usage data. </p> </li> <li> <p>Manage Smart Contracts: The solution shall manage the smart contracts involved in the storage and retrieval of zApps data from the Blockchain platform. This includes their deployment, initialisation and querying. </p> </li> <li> <p>Storage Blockchain transactions: The solution shall store the zApps data in transactions within the Blockchain platform in such a way that it can be queried at a later stage.</p> </li> <li> <p>Request and Usage registration: The solution shall store information regarding when a zApp was acquired, as well as when it is used and by whom.</p> </li> <li> <p>Application Usage Tracking Visualization (Every zApp that a user owns) (Figure 2): Enables users to visually track and analyze the usage patterns and frequency of each zApp they own.</p> </li> <li> <p>Application Usage Tracking Visualization (Specific zApp) (Figure 3): Offers a dedicated interface displaying detailed analytics and usage metrics of a particular zApp, aiding users in understanding its performance through interactive visual representations.</p> </li> </ol>"},{"location":"WP07/UsageTraceability/#3-architecture-diagram","title":"3. Architecture Diagram","text":"<p>Figure 1. Solution Architecture</p> <p>The solution requires data from other modules created in ZDZW, like the marketplace. These modules (in grey in Figure 1) are external to those created in T7.4. However, they are represented in the figure to facilitate the understanding of how the T7.4 modules will be integrated with the rest of the ZDZW platform. Thus, the modules developed for this task are as follows: </p> <ul> <li>Blockchain API: This service allows other modules to interact with the Blockchain. On the one hand, it will include all the services to be able to manage the the Smart Contract that will be requests from the Marketplace once the user acquires a Zapp. On the other hand, the API must store the transactions that come from the use of the Zapp. In addition, it must respond to the requests that come from the Zapp Traceability Monitoring. This API also is going to interact with the services provided by AWS BaaS. </li> <li>Smart Contract Management: This service manages Smart Contract. It will store different types of Smart Contract, according to the condition of use of the Zapp, install, instantiate, execute a Smart Contract and finish once the execution condition has expired. </li> <li>Traceability Data Management: This service manages providing and storing usage data for traceability. Whenever a Zapp is used based on the condition of use that has been selected in the marketplace, this service will store the transaction in the blockchain, and all requests for Zapp usage will also be made through this module. </li> <li>Blockchain: The Blockchain service, it stores zApp usage information, and Smart Contracts for each zApp purchases.  </li> <li>zApp Usage Tracking interface: User interface for usage traceability (front end).  </li> <li>Traceability Gateway: This component enables the integration of the modules designed in T7.4 with the marketplace and the zApps. It provides Software Development Kits (SDKs) that developers can use in their backend solutions to integrate with the traceability solution, and a dedicated interoperability solution based on Node-Redi. This traceability solution allows zApps developers to easily edit and manage integration flows to register zApp backend data into the Blockchain, using a dedicated integration flow runtime component. Integration flows normally combine data ingestion functions, data processing functions, and a blockchain storage function to register new data in the blockchain. Data processing functions allow to ingest data from backend services such as database services (relational or non-relational database), file systems (e.g. CSV or XML files), or messaging services (e.g. MQTT or AMQP brokers). Data transformation functions support the transformation of data to the specific JSON formatted data that the zApp will use to trace usability data. Finally, the blockchain storage function facilitates the connection to the API Blockchain component. </li> </ul>"},{"location":"WP07/UsageTraceability/#4-image-overview","title":"4. Image Overview","text":"<p>The frontend of the usage tracking system is built around two distinct but complementary functionalities. The first functionality allows users to monitor the usage of all applications briefly. This function provides an overview of application usage, allowing users to obtain valuable information in a glimpse. </p> <p>The second functionality focuses on providing detailed information on individual applications. Leveraging this feature, users can access specific and detailed information about the usage patterns of each application. This tailored insight provides a deeper understanding of usage metrics, allowing users a more comprehensive analysis of their application utilization. </p>"},{"location":"WP07/UsageTraceability/#41-usage-tracker-main-zapps","title":"4.1. Usage Tracker: main zApps","text":"<p>As Figure 1 shows, users can track the usage of the different apps they have purchased from the ZDZW Marketplace. Also, they can get a general summary about the usage of the different zApps, and visualize which ones are the most used. Finally, either by clicking each app or by going to custom view, they can get individualized info on each zApp Figure 2</p> <p></p> <p>Figure 2.</p>"},{"location":"WP07/UsageTraceability/#42-usage-tracker-one-zapp","title":"4.2. Usage Tracker: one zApp","text":"<p>Figure 3 shows the information relating to a specific zApp. The individualized view presents information like the chosen payment method, how many uses they have left or how much they have used the zApp during the last months. Additionally, our implementation includes a date filter, allowing users to refine and analyze usage data based on specific timeframes for enhanced insights and tailored assessments.</p> <p></p> <p>Figure 3.</p>"},{"location":"WP07/UsageTraceability/#43-traceability-gateway","title":"4.3. Traceability Gateway","text":"<p>NOTE: [UPV, REVISE AND COMPLETE]</p> <p>Although the Traceability Gateway is a backend module, it also incorporates an interface. Figure 4 shows a mockup of the user interface of the traceability Gateway. The tool provides a toolkit of nodes that can be used to integrate backend functions. Some of the nodes include MQTT clients, HTTP clients or servers, relational database clients, CSV file readers, etc. Through the interface, users can drag a node and drop it in the edition area, and then interconnect nodes to build the logic of the integration with the traceability function as a flow. </p> <p>The provided example illustrates a sample flow, where an MQTT client node (MQTT in) subscribes to a MQTT topic. When new data is published in this topic, the node will generate a message that will be passed to the next node. This function node performs conversions on the incoming data, formatting the data to be traced in the traceability solution. The formatted data is then passed to a Traceability Gateway node that will register the data in the ZDZW blockchain based traceability solution. </p> <p></p> <p>Figure 4. Traceability Gateway</p>"},{"location":"WP07/UsageTraceability/#5-hardware-components","title":"5. Hardware Components","text":"<p>This does not apply to T7.4.</p>"},{"location":"WP07/UsageTraceability/#6-computation-requirements","title":"6. Computation Requirements","text":"<p>Add details on minimum and recommended cpu, ram and storage requirements</p>"},{"location":"WP07/UsageTraceability/#61-blockchain-platform","title":"6.1. Blockchain Platform","text":"<p>The BAAS platform chosen is the one provided by AWS. Using AWS to deploy a Blockchain network requires the combined use of several of its products. Therefore, it must be ensured that all of them are available in the AWS region being used. We will use Ireland, eu-west-1, which is the one that ensures that the data is on EU soil. </p> <p>Thus, Amazon Managed Blockchain (AMB) has been leveraged to create a private Blockchain network based on Hyperdledger Fabric. AMB can be used to create the Hyperledger network, the members, and their peers. However, this service needs Fabric clients that interact with the network (created using the AWS service EC2) and connection management and traffic control (created using the AWS VPC). An overview of the configuration is presented in Figure 5.</p> <p></p> <p>Figure 5. AWS Hyperledger Deployment</p> <p>For this configuration to be successful, there are minimum requirements that both the peer nodes and the client fabric must meet.</p> <ul> <li>The Blockchain instance type of the peer node must be at least of type bc.t3.small.</li> <li> <p>The EC2 instance containing the Fabric Client must be at least of type t3.medium and has been given a storage capacity of 20 GiB. Furthermore, when configuring the EC2 instance, the Amazon Machine Image (AMI) must be chosen, which, in the case of the instance for the Fabric client, must be Amazon Linux. Moreover, among the available images, Amazon Linux 2023 AMI 2023.1.20230825.0 x86_64 HVM kernel-6.1 and 64-bit architecture have been chosen. Finally, to finish configuring the EC2 instance, it is necessary to ensure that the EC2 instance has all the necessary packages, such as Docker, Docker-compose, or Golang. The required versions of those packages are:</p> </li> <li> <p>Docker\u201317.06.2-ce or later</p> </li> <li> <p>Docker-compose\u20131.14.0 or later</p> </li> <li> <p>Go\u20131.14.x</p> </li> </ul> <p>Note: More information and details about the requirements are provided in the Deployment Annex</p>"},{"location":"WP07/UsageTraceability/#62-traceability-gateway","title":"6.2. Traceability Gateway","text":"<p>The deployment of the Traceability Gateway is containerized using Docker, providing a convenient and isolated environment for efficient execution. Below are the specific computation requirements for the Traceability Gateway within a Dockerized deployment:</p> <ul> <li> <p>Processor (CPU): The Traceability Gateway Docker container is optimized for containerized environments and adapts well to varying CPU resources. A quad-core or higher processor is recommended for handling increased workloads and concurrent integration flows.</p> </li> <li> <p>Random Access Memory (RAM): Allocate a minimum of 8GB of RAM to the Docker container hosting the Traceability Gateway.</p> </li> <li> <p>Storage: Docker containers benefit from efficient storage utilization. Ensure that the host system provides sufficient disk space for Docker images and containers. A minimum of 20GB of available disk space is recommended to accommodate configuration files, logs, and temporary data within the Traceability Gateway container.</p> </li> <li> <p>Operating System Compatibility: Docker containers offer platform-agnostic deployment. The Traceability Gateway Docker image is compatible with various operating systems. Users can deploy the Docker container on Linux, Windows, or macOS systems that support Docker. Ensure that the host system has Docker installed and configured.</p> </li> </ul> <p>By considering these computation requirements in a Dockerized environment, users can optimize the deployment of the Traceability Gateway, leveraging the benefits of containerization for efficient integration and management of zApps traceability within the ZDZW platform.</p>"},{"location":"WP07/UsageTraceability/#63-usage-tracking-visualization","title":"6.3. Usage tracking Visualization","text":"<p>The Usage tracking Visualization is a web application that uses Angular framework to display data and charts. It runs inside a Docker container, which makes it easy to deploy and manage. To run the Usage tracking Visualization, a server with Docker installed will be needed and a hardware of at least 2 cpu cores and 4Gb of RAM for optimal performance.</p>"},{"location":"WP07/UsageTraceability/#7-installation-procedure","title":"7. Installation Procedure","text":"<p>Step by step on how to install the application: * Standalone * In the Kubernetes platformm using helm charts: description of the different options</p>"},{"location":"WP07/UsageTraceability/#71-blockchain-platform","title":"7.1. Blockchain Platform","text":"<p>Once the minimum configuration explained in section 6.1 has been set up, connections to the Blockchain network can be established. For the first connection, the following are required: - A Blockchain network. - A channel within this network. - An EC2 instance. - The security zones explained in section 6.1 correctly configured.</p> <p>For a successful first connection from a local computer, it is necessary to have:  - The private key used during the creation of the VPCs. - The public IP of the computer from which we are connecting. - The public DNS of the instance.</p> <p>To control incoming traffic to the EC2 instance, the public IP of the local machine will be permitted to connect to the VPC. Once permissions are granted, using SSH and the public DNS of the instance, it is possible to connect to the instance remotely.</p> <p>Once on the instance, make sure the Docker container containing the fabric client is running:</p> <pre><code>docker-compose -f docker-compose-cli.yaml up -d```\n</code></pre> <p>Once the fabric client is up, packages can be installed and uninstalled in the instance like in a regular Ubuntu machine. As far as the installation and deployment of Smart contracts are concerned, this is done via dockerized .go scripts. A detailed explanation can be found in the Annex.</p>"},{"location":"WP07/UsageTraceability/#72-traceability-gateway","title":"7.2. Traceability Gateway","text":"<p>To deploy the Traceability Gateway using Docker Compose from the provided Git repository and Helm charts for Kubernetes (to be released in the future), follow these steps:</p> <ol> <li> <p>Clone the Git Repository: <code>bash    git clone https://github.com/zdzw-eu/traceability-gateway.git</code></p> </li> <li> <p>Navigate to the Repository Directory: <code>bash    cd traceability-gateway</code></p> </li> <li> <p>Configure Docker Compose (Optional):    Modify the <code>docker-compose.yml</code> file if specific configurations are required, such as adjusting ports or volumes.</p> </li> <li> <p>Build and Start the Docker Containers: <code>bash    docker-compose up -d</code>    This command builds and starts the Traceability Gateway Docker containers in detached mode.</p> </li> <li> <p>Access the User Interface:    Open a web browser and navigate to <code>http://localhost:1880</code> to access the Node-RED user interface.</p> </li> <li> <p>Configure Integration Flows:    Utilize the Node-RED interface to design and manage integration flows. Customize nodes, interconnect them, and configure each node based on your integration requirements.</p> </li> <li> <p>Save and Deploy Flows:    Save your integration flows within Node-RED, and click the \"Deploy\" button to activate the changes. This ensures that your configured flows are ready for execution.</p> </li> <li> <p>Monitor Logs:    Monitor the container logs for any potential issues or debug information. Use the following command to view logs:    <code>bash    docker-compose logs -f</code></p> </li> <li> <p>Shutdown the Containers:    When needed, stop and remove the Traceability Gateway containers:    <code>bash    docker-compose down</code></p> </li> </ol> <p>By following these steps with Docker Compose, you can deploy the Traceability Gateway from the specified Git repository, facilitating seamless integration and management of zApps traceability within the ZDZW platform. Adjust configurations as necessary for your specific deployment environment and requirements.</p> <p>Note for Kubernetes Users: Helm charts for deploying the Traceability Gateway on Kubernetes are currently under development and will be released in the future. Stay tuned for updates on Helm chart availability. Adjust configurations as necessary for your specific deployment environment and requirements.</p>"},{"location":"WP07/UsageTraceability/#73-usage-tracking-visualization","title":"7.3. Usage tracking Visualization","text":"<p>To get started, clone the project and set up the environment variables. Make sure you enter the correct URLs of the services that the web app will communicate with. Then build and start docker containers generated with this command:    <code>bash    docker-compose up -d</code></p>"},{"location":"WP07/UsageTraceability/#8-how-to-use","title":"8. How To Use","text":""},{"location":"WP07/UsageTraceability/#81-blockchain-platform","title":"8.1. Blockchain Platform","text":"<p>The Blockchain's data storage and retrieval services are provided by a Blockchain API, as shown in Figure 1. This API exposes the Blockchain services to the Traceability Gateway and the zApp usage tracking to facilitate the integration of all the modules that make up T7.4.</p> <p>This API is deployed on the AWS instance, as the Computation Requirements section explains. The connection to this API is made via a public IP of the instance through port 8080. Access privileges have been given to the Traceability Gateway and the zApp usage tracking to allow access. This ensures privacy and access control to the security groups interacting with the Blockchain platform, which, in turn, enhances the privacy of the solution.</p> <p>Depending on the business model of each Zapp, the smart contract will be different. Four smart contract models have been defined. Currently the \"Pay per Volume\" model has been implemented, so the examples below refer to this type of contract.</p> <p>The data offered is in JSON format, which corresponds to the following schema:</p> <pre><code>{\n \"ZAppId\":\"ZApp1\",\n \"UserId\":\"User1\",\n \"MachineId\":\"Machine1\",\n \"SCtype\":\"A\",\n\u00a0 \"UsageInfo\": [\n     {\"UsesLeft\":\"100\", \"InitTime\":\"2023-01-01\", \"EndTime\":\"\"},\n     {\"UsesLeft\":\"95\", \"InitTime\":\"2023-01-02\", \"EndTime\":\"2023-01-03\"}\n  ]\n}\n</code></pre> <p>By exploiting this data format, T7.4 modules can invoke smart contracts, and use them to obtain and store information. For this purpose, the API offers five calls, which are as follows:</p> <p>Create a SmartContract for a Zapp:</p> <p>When a company buys a Zapp using the marketplace the sentences that the MarketPlace will invoke is this one:</p> <pre><code>curl http://3.248.77.23:8080/createSmartContractZapp --include --header \"Content-Type: application/json\" --request \"POST\" --data '{\"ZAppId\": \"ZApp1\", \"UserId\":\"User1\", \"PayType\":\"A\", \"MaxUses\":\"500\",\"InitTime\":\"2024-01-01 01:01:01\"}'\n\n</code></pre> <p>Whose response, if correct is \"initTrue\"</p> <p>Initializating the MaxUses of a Zapp in a Machine:</p> <pre><code>curl http://3.248.77.23:8080/invokeInit --include --header \"Content-Type: application/json\" --request \"POST\" --data '{\"ZAppId\": \"ZApp1\", \"UserId\":\"User1\", \"MachineId\":\"Machine1\", \"PayType\":\"A\", \"MaxUses\":\"500\",\"InitTime\":\"2024-01-01 01:01:01\"}'\n\n</code></pre> <p>Whose response, if correct is \"initTrue\"</p> <p>Store Use data in the Blockchain:</p> <pre><code>curl http://3.248.77.23:8080/invokeInfo --include --header \"Content-Type: application/json\" --request \"POST\" --data '{\"ZAppId\":\"ZApp1\", \"UserId\":\"User1\", \"MachineId\":\"Machine1\", \"PayType\":\"A\", \"UsesCount\":\"10\", \"InitTime\":\"2024-01-01 01:01:01\", \"EndTime\":\"2024-01-28 01:01:01\"}' \n</code></pre> <p>Whose response, if correct is \"true\"</p> <p>Get Max Uses Initialization Data</p> <pre><code>curl http://3.248.77.23:8080/getInitInfo/ --include --header \"Content-Type: application/json\" --request \"POST\" --data '{\"ZAppId\": \"ZApp1\", \"UserId\":\"User1\", \"MachineId\": \"Machine1\", \"PayType\":\"A\"}' \n</code></pre> <p>Get all the historical data of the use of a Zapp</p> <pre><code>curl http://3.248.77.23:8080/getAllInfo --include --header \"Content-Type: application/json\" --request \"POST\" --data '{\"ZAppId\":\"ZApp1\", \"UserId\":\"User1\", \"MachineId\":\"Machine1\", \"PayType\":\"A\", \"InitTime\":\"2016-01-01 01:01:01\", \"EndTime\":\"2025-01-01 01:01:01\"}' \n</code></pre>"},{"location":"WP07/UsageTraceability/#82-traceability-gateway","title":"8.2. Traceability Gateway","text":"<p>The Traceability Gateway offers a user-friendly interface through Node-RED, providing a visual and intuitive way to configure integration flows for zApps traceability. Follow these steps to effectively use the Traceability Gateway:</p> <ol> <li> <p>Access the Node-RED Interface:    Open a web browser and navigate to <code>http://localhost:1880</code> or the specified host and port where Node-RED is running. This grants access to the Node-RED user interface.</p> </li> <li> <p>Explore the Node Palette:    On the left side of the interface, you'll find the node palette containing various nodes representing different functionalities tailored for different smart contract types.</p> </li> <li> <p>Drag and Drop Nodes:  Build integration flows by dragging nodes from the palette and dropping them onto the editing area. This allows you to visually design the flow of data for traceability within the ZDZW platform.</p> </li> <li> <p>Interconnect Nodes:    Connect nodes by drawing links between them to define the logical sequence of data processing. This interconnection forms the integration flow, ensuring a smooth transition of data from one node to another.</p> </li> <li> <p>Configure Nodes:    Double-click on each node to configure its parameters according to your specific use case. Configure settings such as connection details, data transformations, and other parameters required for traceability.</p> </li> <li> <p>Deploy Integration Flows:    Save your configured integration flows within Node-RED, and click the \"Deploy\" button. This action activates the changes and makes your integration flows operational, ready to handle zApps traceability data.</p> </li> <li> <p>Monitor Execution:    Use the Node-RED interface to monitor the execution of integration flows. The interface provides real-time feedback on the status of nodes, ensuring transparency in the data processing pipeline.</p> </li> <li> <p>Troubleshoot and Debug:    In case of issues, utilize the Node-RED interface to troubleshoot and debug integration flows. Examine logs, inspect node statuses, and make adjustments as needed to ensure smooth operation.</p> </li> <li> <p>Iterate and Improve:    Continuously iterate on your integration flows based on evolving requirements. Use the flexibility of the Node-RED interface to make improvements, add new nodes, or modify existing configurations for enhanced traceability.</p> </li> <li> <p>Save and Export Flows (Optional):     Save your integration flows as a project within Node-RED, allowing for easy retrieval and modification. Optionally, export flows for sharing with other developers or for backup purposes.</p> </li> </ol> <p>By following these steps, you can effectively use the Traceability Gateway through Node-RED, enabling seamless integration and management of zApps traceability within the ZDZW platform. Adjust configurations as necessary to meet specific traceability requirements.</p> <p>For example, users can integrate Node-RED with the Traceability Gateway and a SQL database, enabling them to orchestrate a robust and customizable workflow for efficient management of Zapp usage data within the ZDZW platform. The visual representation of the flow not only simplifies the design process but also facilitates easy monitoring and maintenance of intricate integration processes. As depicted in Figure 6, the interconnected nodes and logical sequence provide a clear overview, empowering users to visualize and optimize their data processing pipeline effortlessly.</p> <p> Figure 6. Traceability Gateway</p>"},{"location":"WP07/UsageTraceability/#83-usage-tracking-visualization","title":"8.3. Usage tracking Visualization","text":"<p>For the Usage Tracking Visualization tool, users need to enter the URL of the web-app in their browser and log in with their valid credentials. The tool provides interactive features such as filtering, sorting, zooming data and charts.</p>"},{"location":"WP07/UsageTraceability/#9-additional-learning-materials","title":"9. Additional Learning Materials","text":"<p>HiperLedged Fabric https://www.hyperledger.org/projects/fabric</p> <p>Amazon Managed Blockchain https://aws.amazon.com/es/managed-blockchain/ </p> <p>Amazon Managed Blockchain Deployment https://github.com/zdzw-eu/UsageTraceability/blob/main/docs/annex/ZDZW_AmazonManagedBlockhainDeployment_ANNEX.pdf</p>"},{"location":"WP07/ValuechainAppStore/","title":"Valuechain AppStore","text":""},{"location":"WP07/ValuechainAppStore/#general-description","title":"General Description","text":"<p>The Valuechain AppStore is part of Valuechain's Network Portal platform. Network Portal is a dynamic networking platform that offers cluster and network management. insights. and streamlined communications. The portal captures intelligence and promotes collaboration with easy two-way communications features. Its AppStore will make various applications avialble to its users/cpmpanies/clusters to help improve productivity and efficiency. </p> <p>The Valuechain AppStore will interlink with ZDZW (and its marketplace). adding value to extend ZDZW to Valuechain's existing commercialised platform where manufacturing companies and industry associations were already using the platform on day-to-day basis. It will allow end-users to utilise the unique offerings (services/tools/applications/products) from both Valuechain and ZDZW seamlessly. </p>"},{"location":"WP07/ValuechainAppStore/#top-ten-functionalities","title":"Top Ten Functionalities","text":"<ol> <li>Adapt Valuechain AppStore to allow unregistered user to see products/tools/services/applications</li> <li>Integration with ZDZW to avoid login/register for both marketplace/AppStote</li> <li>API to interlink with ZDZW marketplace</li> <li>Create/upload/link/edit/de-activate/delist a product/tool/service/application in AppStore</li> <li>List and filter the products in the Valuechain AppStore by categories</li> <li>Search in Valuechain AppStore</li> <li>Allow supplier(seller) add/edit company information to the Valuechain AppStore as interested buyers/customers may want to check their public information. Similar for the customer who want to buy. they can add company information to VLC Marketplace which might help them find what they need via VLC network portal's recommendation engine.</li> <li>Integration of the Stripe (e.g. connect user Stripe account when they register app so that they can receive payments via Stripe Connect). if commission needs to be automatically calculated. the transaction flow will be clarified. implemented and tested when implementing stripe integration.</li> <li>Provide VLC AppStore documentation and instructions to help users with the product creation/upload workflow</li> <li>Logging transactions -- Could be integration with the ZDZW blockchain logging service for the app usage/purchase transitions, or Valuechain AppStore's tracking logs)</li> </ol> <p>note: Marketplace and AppStore are interchangeable in terms of the WP7 marketplace development</p>"},{"location":"WP07/ValuechainAppStore/#architecture-diagram","title":"Architecture Diagram","text":"<p>The high level Architecture diagram from the Sofware Specification document  Figure 1</p> <p>The indicative work flow (based on different user role) on Valuechain's Network Portal.   Figure 2</p>"},{"location":"WP07/ValuechainAppStore/#image-overview","title":"Image Overview","text":"<p>The following are the new UI design in the 4th quater of 2023.</p> <p>Landing Page:  Figure 3</p> <p>More options when clicking on the three dots on the top-right corner (next to 'Add new application')</p> <p></p> <p>Figure 4</p> <p>My Apps page:  Figure 5</p> <p>Individual App's page (example only at the moment, it will include other details of the App that have been input in the 'Add new application' window)  Figure 6</p> <p>Individual App's details filling page (when creating/publishing the app)  Figure 7</p> <p>Individual App's details filling page  -- where uploaded images can ve previewed  Figure 8</p> <p>Message when user submitted the app details  Figure 9</p>"},{"location":"WP07/ValuechainAppStore/#hardware-components","title":"Hardware Components","text":"<p>N/A as Valuechain AppStore is deployed on Miscrosoft Azure along with its commerical platform</p>"},{"location":"WP07/ValuechainAppStore/#computation-requirements","title":"Computation Requirements","text":"<p>N/A as Valuechain AppStore is deployed on Miscrosoft Azure along with its commerical platform</p>"},{"location":"WP07/ValuechainAppStore/#installation-procedure","title":"Installation Procedure","text":"<p>Valuechain's AppStore is delpyed with Valuechain Network Portal on the cloud server serving as SaaS based application. </p>"},{"location":"WP07/ValuechainAppStore/#how-to-use","title":"How To Use","text":"<p>note: details on how to access the AppStore will be added later once UI implementation is completed * For accessing Valuechain Network Portal directly:   * Testing Environment https://test.valuechain.com/Login   * Production Environment https://my.valuechain.com/Login</p> <ul> <li>For interlinking in the backend, the following APIs are used. Corresding API access document has been shared with WP7 partners (task T7.2 and T7.3).</li> <li>Testing Environment https://testapi.valuechain.com</li> <li>Production Environment https://api.valuechain.com</li> </ul>"},{"location":"WP07/ValuechainAppStore/#additional-learning-materials","title":"Additional Learning Materials","text":"<p>Links to other learning materials like youtube tutorials or work from WP10</p>"}]}